[üá®üá≥](/README-cn.md "Simplified Chinese")
[üáØüáµ](/README-ja.md "Japanese")
[üáÆüáπ](/README-it.md "Italian")
[üá∞üá∑](/README-ko.md "Korean")

[![license](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)](https://img.shields.io/badge/license-BSD--3--Clause-blue.svg)

# –ò–Ω—Ç—Ä–æ
–ù–µ–∂–Ω–æ–µ –∏–Ω—Ç—Ä–æ –∫ –≤–∏–¥–µ–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω–æ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–∏—Å—Ç–æ–≤ –∏ –∏–Ω–∂–µ–Ω–µ—Ä–æ–≤, —Ö–æ—Ç—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Ç—É—Ç –∏–∑—É—á–∞–µ–º–∞ **–ª—é–±–æ–º—É** –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ–º—É. –≠—Ç–∞ –∏–¥–µ—è —Ä–æ–¥–∏–ª–∞—Å—å –≤–æ –≤—Ä–µ–º—è [–º–∏–Ω–∏-—Å–µ–º–∏–Ω–∞—Ä–∞ –¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ –≤–∏–¥–µ–æ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π](https://docs.google.com/presentation/d/17Z31kEkl_NGJ0M66reqr9_uTG6tI5EDDVXpdPKVuIrs/edit#slide=id.p).

–¶–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –≤–∏–¥–µ–æ **–ø—Ä–æ—Å—Ç—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏, —Å—Ö–µ–º–∞–º–∏ –∏ –ø—Ä–∞–∫—Ç–∏–∫–æ–π**. –ù–µ —Å—Ç–µ—Å–Ω—è–π—Ç–µ—Å—å –¥–æ–±–æ–≤–ª—è—Ç—å —Å–≤–æ–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è —É–ª—É—á—à–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞.

**–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–∞–∑–¥–µ–ª—ã** —Ç—Ä–µ–±—É—é—Ç —á—Ç–æ–±—ã —É –≤–∞—Å –±—ã–ª —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω **–î–æ–∫–µ—Ä** –∏ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω —ç—Ç–æ—Ç —Ä–µ–ø–æ.

```bash
git clone https://github.com/leandromoreira/digital_video_introduction.git
cd digital_video_introduction
./setup.sh
```

> **–ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï**: –∫–æ–≥–¥–∞ –≤—ã –≤–∏–¥–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É `./s/ffmpeg` –∏–ª–∏`./s/mediainfo`, —ç—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º—ã –∑–∞–ø—É—Å–∫–∞–µ–º **–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–Ω—É—é –≤–µ—Ä—Å–∏—é** —ç—Ç–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã, –∫–æ—Ç–æ—Ä–∞—è —É–∂–µ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è.

–í—Å–µ **–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è –∏–∑ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø–∞–ø–∫–∏**. –î–ª—è **–ø—Ä–∏–º–µ—Ä–æ–≤ jupyter** –≤—ã –¥–æ–ª–∂–Ω—ã –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä `./s/start_jupyter.sh`, –∏ –ø–æ—Å–µ—Ç–∏—Ç—å URL –≤ –±—Ä–∞—É–∑–µ—Ä–µ.

# Changelog

* added DRM system
* released version 1.0.0
* added simplified Chinese translation

# –ò–Ω–¥–µ–∫—Å

- [–ò–Ω—Ç—Ä–æ](#–∏–Ω—Ç—Ä–æ)
- [–ò–Ω–¥–µ–∫—Å](#–∏–Ω–¥–µ–∫—Å)
- [–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã](#–æ—Å–Ω–æ–≤–Ω—ã–µ-—Ç–µ—Ä–º–∏–Ω—ã)
  * [–î—Ä—É–≥–∏–µ —Å–ø–æ—Å–æ–±—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Ü–≤–µ—Ç–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è](#–¥—Ä—É–≥–∏–µ-—Å–ø–æ—Å–æ–±—ã-–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
  * [–ü—Ä–∞–∫—Ç–∏–∫–∞: –µ–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º —Ü–≤–µ—Ç–∞–º–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏](#–ø—Ä–∞–∫—Ç–∏–∫–∞-–µ–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º-—Ü–≤–µ—Ç–∞–º–∏-–∏-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏)
  * [DVD —ç—Ç–æ DAR 4:3](#dvd-—ç—Ç–æ-dar-43)
  * [–ü—Ä–∞–∫—Ç–∏–∫–∞: –†–∞—Å—Å–º–æ—Ç—Ä —Å–≤–æ–π—Å–≤—Ç–≤–∞ –≤–∏–¥–µ–æ](#–ø—Ä–∞–∫—Ç–∏–∫–∞-—Ä–∞—Å—Å–º–æ—Ç—Ä-—Å–≤–æ–π—Å—Ç–≤–∞-–≤–∏–¥–µ–æ)
- [–£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏](#—É–¥–∞–ª–µ–Ω–∏–µ-–∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏)
  * [–¶–≤–µ—Ç–∞, —è—Ä–∫–æ—Å—Ç—å –∏ –≥–ª–∞–∑–∞](#—Ü–≤–µ—Ç–∞-—è—Ä–∫–æ—Å—Ç—å-–∏-–≥–ª–∞–∑–∞")
    + [–ú–æ–¥–µ–ª—å —Ü–≤–µ—Ç–∞](#–º–æ–¥–µ–ª—å-—Ü–≤–µ—Ç–∞)
    + [–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –º–µ–∂–¥—É YCbCr –∏ RGB](#–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ-–º–µ–∂–¥—É-ycbcr-–∏-rgb)
    + [–¶–≤–µ—Ç–æ–≤–∞—è —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è](#—Ü–≤–µ—Ç–æ–≤–∞—è-—Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è)
    + [–ü—Ä–∞–∫—Ç–∏–∫–∞: –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã YCbCr](#–ø—Ä–∞–∫—Ç–∏–∫–∞-–ø—Ä–æ–≤–µ—Ä–∫–∞-–≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã-ycbcr)
  * [–¢–∏–ø—ã –∫–∞–¥—Ä–æ–≤](#—Ç–∏–ø—ã-–∫–∞–¥—Ä–æ–≤)
    + [–ò –∫–∞–¥—Ä (–∏–Ω—Ç—Ä–∞, –∫–ª—é—á–µ–≤–æ–π –∫–∞–¥—Ä)](#–∏-–∫–∞–¥—Ä-–∏–Ω—Ç—Ä–∞-–∫–ª—é—á–µ–≤–æ–π-–∫–∞–¥—Ä)
    + [–ü –∫–∞–¥—Ä (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π)](#–ø-–∫–∞–¥—Ä-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π)
      - [–ü—Ä–∞–∫—Ç–∏–∫–∞: –í–∏–¥–µ–æ —Å –µ–¥–∏–Ω—ã–º I-–∫–∞–¥—Ä–æ–º](#–ø—Ä–∞–∫—Ç–∏–∫–∞-–≤–∏–¥–µ–æ-—Å-–µ–¥–∏–Ω—ã–º-i-–∫–∞–¥—Ä–æ–º)
    + [–ë –∫–∞–¥—Ä (–¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)](#–±-–∫–∞–¥—Ä-–¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–≥–æ-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)
      - [–ü—Ä–∞–∫—Ç–∏–∫–∞: –°—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ —Å –ë-–∫–∞–¥—Ä–∞–º–∏](#–ø—Ä–∞–∫—Ç–∏–∫–∞-—Å—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ-–≤–∏–¥–µ–æ-—Å-–±-–∫–∞–¥—Ä–∞–º–∏)
    + [–ê–Ω–Ω–æ—Ç–∞—Ü–∏—è](#–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è)
  * [–í—Ä–µ–º–µ–Ω–Ω–∞—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å (–º–µ–∂ –ø—Ä–µ–¥—Å–∫–æ–∑–∞–Ω–∏–µ)](#–≤—Ä–µ–º–µ–Ω–Ω–∞—è-–∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å-–º–µ–∂-–ø—Ä–µ–¥—Å–∫–æ–∑–∞–Ω–∏–µ)
      - [–ü—Ä–∞–∫—Ç–∏–∫–∞: –û–±–∑–æ—Ä –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–≤–∏–∂–µ–Ω–∏—è](#–ø—Ä–∞–∫—Ç–∏–∫–∞-–æ–±–∑–æ—Ä-–≤–µ–∫—Ç–æ—Ä–æ–≤-–¥–≤–∏–∂–µ–Ω–∏—è)
  * [–ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)](#–ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è-–∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å-–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ)
      - [–ü—Ä–∞–∫—Ç–∏–∫–∞: –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π](#–ø—Ä–∞–∫—Ç–∏–∫–∞-–ø—Ä–æ–≤–µ—Ä–∫–∞-–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π)
- [–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤–∏–¥–µ–æ–∫–æ–¥–µ–∫?](#–∫–∞–∫-—Ä–∞–±–æ—Ç–∞–µ—Ç-–≤–∏–¥–µ–æ–∫–æ–¥–µ–∫)
  * [–ß—Ç–æ? –ü–æ—á–µ–º—É? –ö–∞–∫?](#—á—Ç–æ-–ø–æ—á–µ–º—É-–∫–∞–∫)
  * [–ò—Å—Ç–æ—Ä–∏—è](#–∏—Å—Ç–æ—Ä–∏—è)
    + [–†–æ–∂–¥–µ–Ω–∏–µ AV1](#—Ä–æ–∂–¥–µ–Ω–∏–µ-av1)
  * [–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–æ–¥–µ–∫](#—É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π-–∫–æ–¥–µ–∫)
  * [1-–π —à–∞–≥ - —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π](#1-–π-—à–∞–≥---—Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)
    + [–ü—Ä–∞–∫—Ç–∏–∫–∞: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–æ–≤](#–ø—Ä–∞–∫—Ç–∏–∫–∞-–ø—Ä–æ–≤–µ—Ä–∫–∞-—Ä–∞–∑–¥–µ–ª–æ–≤)
  * [2-–π —à–∞–≥ - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è](#2-–π-—à–∞–≥---–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)
  * [3-–π —à–∞–≥ - –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è](#3-–π-—à–∞–≥---–ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è)
    + [–ü—Ä–∞–∫—Ç–∏–∫–∞: –≤—ã–±—Ä–∞—Å—ã–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤](#–ø—Ä–∞–∫—Ç–∏–∫–∞-–≤—ã–±—Ä–∞—Å—ã–≤–∞–Ω–∏–µ-—Ä–∞–∑–Ω—ã—Ö-–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤)
  * [4-–π —à–∞–≥ - –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ](#4-–π-—à–∞–≥---–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ)
    + [–ü—Ä–∞–∫—Ç–∏–∫–∞: –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ](#–ø—Ä–∞–∫—Ç–∏–∫–∞-–∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ)
  * [5-–π —à–∞–≥ - —ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ](#5-–π-—à–∞–≥---—ç–Ω—Ç—Ä–æ–ø–∏–π–Ω–æ–µ-–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ)
    + [VLC-–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ](#vlc-–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ)
    + [–ê—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ](#–∞—Ä–∏—Ñ–º–µ—Ç–∏—á–µ—Å–∫–æ–µ-–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ)
    + [–ü—Ä–∞–∫—Ç–∏–∫–∞: CABAC vs CAVLC](#–ø—Ä–∞–∫—Ç–∏–∫–∞-cabac-vs-cavlc)
  * [6-–π —à–∞–≥ - —Ñ–æ—Ä–º–∞—Ç –±–∏—Ç–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫–∞](#6-–π-—à–∞–≥---—Ñ–æ—Ä–º–∞—Ç-–±–∏—Ç–æ–≤–æ–≥–æ-–ø–æ—Ç–æ–∫–∞)
    + [H.264 –±–∏—Ç–æ–≤—ã–π –ø–æ—Ç–æ–∫](#h264-–±–∏—Ç–æ–≤—ã–π-–ø–æ—Ç–æ–∫)
    + [–ü—Ä–∞–∫—Ç–∏–∫–∞: –ü—Ä–æ–≤–µ—Ä–π–∞ –±–∏—Ç–æ–≤–æ–≥–∞ –ø–æ—Ç–æ–∫–∞ H.264](#–ø—Ä–∞–∫—Ç–∏–∫–∞-–ø—Ä–æ–≤–µ—Ä–π–∞-–±–∏—Ç–æ–≤–æ–≥–∞-–ø–æ—Ç–æ–∫–∞-h264)
  * [–û–±–∑–æ—Ä](#o–±–∑–æ—Ä)
  * [–ö–∞–∫ H.265 –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ª—É—á—à—É—é —Å—Ç–µ–ø–µ–Ω—å —Å–∂–∞—Ç–∏—è —á–µ–º H.264?](#–∫–∞–∫-h265-–æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç-–ª—É—á—à—É—é-—Å—Ç–µ–ø–µ–Ω—å-—Å–∂–∞—Ç–∏—è-—á–µ–º-h264)
- [–û–Ω–ª–∞–π–Ω —Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è](#–æ–Ω–ª–∞–π–Ω-—Ç—Ä–∞–Ω—Å–ª—è—Ü–∏—è)
  * [–û–±—â–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞](#–æ–±—â–∞—è-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞)
  * [–ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ø–µ—Ä–µ–¥–∞—á–∞](#–ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è-–∑–∞–≥—Ä—É–∑–∫–∞-–∏-–∞–¥–∞–ø—Ç–∏–≤–Ω–∞—è-–ø–µ—Ä–µ–¥–∞—á–∞)
  * [–ó–∞—â–∏—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞](#–∑–∞—â–∏—Ç–∞-–∫–æ–Ω—Ç–µ–Ω—Ç–∞)
- [–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å jupyter](#–∫–∞–∫-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å-jupyter)
- [–ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏](#–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏)
- [–°—Å—ã–ª–∫–∏](#—Å—Å—ã–ª–∫–∏)

# –û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã

**–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ** –º–æ–∂–Ω–æ –ø—Ä–µ–¥—Å—Ç–æ–≤–ª—è—Ç—å –≤ —Ñ–æ—Ä–º–µ **2–î –º–∞—Ç—Ä–∏—Ü—ã**. –ï—Å–ª–∏ –º—ã –¥—É–º–∞–µ–º –æ **—Ü–≤–µ—Ç–∞—Ö**, —Ç–æ –º–æ–∂–Ω–æ –µ–∫—Å—Ç—Ä–æ–ø–∞–ª–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–∏–Ω–∏–µ –≤ **3–î –º–∞—Ç—Ä–∏—Ü—É**, –≥–¥–µ **–¥–æ–±–∞–≤–æ—á–Ω—ã–µ –∏–∑–º–µ—Ä–µ–Ω–∏—è** –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç **—Ü–≤–µ—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é**.

–ï—Å–ª–∏ –º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ–º –µ—Ç–∏ —Ü–≤–µ—Ç–∞ [–ø–µ—Ä–≤–∏—á–Ω—ã–º–∏ —Ü–≤–µ—Ç–∞–º–∏ (–∫—Ä–∞—Å–Ω—ã–π, –∑–µ–ª–µ–Ω—ã–π, —Å–∏–Ω–∏–π](https://en.wikipedia.org/wiki/Primary_color), –º—ã –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç—Ä–∏ —É—Ä–æ–≤–Ω—è, –æ–¥–∏–Ω –¥–ª—è **–∫—Ä–∞—Å–Ω–æ–≥–æ**, –æ–¥–∏–Ω –¥–ª—è **–∑–µ–ª–µ–Ω–æ–≥–æ**, –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–ª—è **—Å–∏–Ω–µ–≥–æ**.

![–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —ç—Ç–æ 3–î –º–∞—Ç—Ä–∏—Ü–∞](/i/image_3d_matrix_rgb.png "–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —ç—Ç–æ 3–î –º–∞—Ç—Ä–∏—Ü–∞")

–ú—ã –Ω–∞–∑–æ–≤–µ–º –∫–∞–∂–¥—É—é —Ç–æ—á–∫—É –Ω–∞ –µ—Ç–æ–π –º–∞—Ç—Ä–∏—Ü—ã **–ø–∏–∫—Å–µ–ª–µ–º** (–µ–ª–µ–º–µ–Ω—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è). –û–¥–∏–Ω –ø–∏–∫—Å–µ–ª –æ—Ç—Ä–∞–∂–∞–µ—Ç **–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å** (–æ–±—ã—á–Ω–æ —á–∏—Å–ª–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ) –¥–∞–Ω–Ω–æ–≥–æ —Ü–≤–µ—Ç–∞. –ù–∞–ø—Ä–∏–º–µ—Ä, **–∫—Ä–∞—Å–Ω–∏–π –ø–∏–∫—Å–µ–ª—å** –∏–º–µ–µ—Ç 0 –∑–µ–ª–µ–Ω–æ–≥–æ, 0 —Å–∏–Ω–µ–≥–æ, –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫—Ä–∞—Å–Ω–æ–≥–æ. **–†–æ–∑–æ–≤—ã–π –ø–∏–∫—Å–µ–ª—å** —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ —Ç—Ä–µ—Ö —Ü–≤–µ—Ç–æ–≤ - **–ö—Ä–∞—Å–Ω—ã–π=255, –ó–µ–ª–µ–Ω—ã–π=192, –°–∏–Ω–∏–∏–π=203**.

> #### –î—Ä—É–≥–∏–µ —Å–ø–æ—Å–æ–±—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
> –°—É—â–µ—Å—Ç–≤—É–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏–π –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–∏—Å–æ–≤–∞—é—Ç –∫–∞–∫ —Ä–∞—Å–ø–ø—Ä–µ–¥–µ–ª—è—é—Ç—å—Å—è —Ü–≤–µ—Ç–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ. –ù–∞–ø—Ä–∏–º–µ—Ä, –≤ –º–µ—Å—Ç–æ —Ç—Ä–µ—Ö –±–∏—Ç–æ–≤, –∫–∞–∫ –¥–ª—è –º–æ–¥–µ–ª–∏ RGB (–ö—Ä–∞—Å–Ω—ã–π, –ó–µ–ª–µ–Ω—ã–π, –°–∏–Ω–∏–π) –º—ã –º–æ–∂–µ–º –∏–∑–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –±–∏—Ç. –ï—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –º–µ–Ω—å—à–µ –ø–∞–º—è–∞—Ç–∏ –Ω–æ —Ç–∞–∫ –∂–µ –¥–∞–µ—Ç –º–µ–Ω—å—à—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ü–≤–µ—Ç–æ–≤.

>
> ![—Å–±–æ—Ä —Ü–≤–µ—Ç–æ–≤ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ NES](/i/nes-color-palette.png "—Å–±–æ—Ä —Ü–≤–µ—Ç–æ–≤ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ NES")

–ù–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–Ω–∏–∑—É, –ø–µ—Ä–≤—ã–π —Å–Ω–∏–º–æ–∫ –æ—Ç—Ä–∞–∂–∞–µ—Ç –≤—Å–µ —Ü–≤–µ—Ç–æ–≤—ã–µ –∫–∞–Ω–∞–ª—ã. –û—Å—Ç–∞–ª—å–Ω—ã–µ —Å–Ω–∏–º–∫–∏ —Ç–æ–ª—å–∫–æ –æ—Ç—Ä–∞–∂–∞—è—é—Ç –∫—Ä–∞—Å–Ω—ã–π, –∑–µ–ª–µ–Ω—ã–π –∏ —Å–∏–Ω–∏–π –∫–∞–Ω–∞–ª.

![–ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å RGB –∫–∞–Ω–∞–ª–æ–≤](/i/rgb_channels_intensity.png "–ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å RGB –∫–∞–Ω–∞–ª–æ–≤intensity")

–ú—ã –≤–∏–¥–µ–º —á—Ç–æ **–∫—Ä–∞—Å–Ω–∏–π –∫–∞–Ω–∞–ª** –±—É–¥–µ—Ç —Å–∞–º—ã–º —è—Ä–∫–∏–º (–±–µ–ª—ã–µ —á–∞—Å—Ç–∏ –≤—Ç–æ—Ä–æ–≥–æ –ª–∏—Ü–∞), –∏ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É–µ—Ç –≥–ª–∞–≤–Ω–æ–º—É –æ—Ç—Ç–µ–Ω–∫—É –≤ –∫–∞—Ä—Ç–∏–Ω–∫–µ. **–°–∏–Ω–µ–≥–æ** –º–∞–ª–æ, –∏ –≤–∏–¥–Ω–æ –≤ —á–µ—Ç–≤–µ—Ä—Ç–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ —á—Ç–æ –æ–Ω **—Å–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω –≤ –≥–ª–∞–∑–∞—Ö –∏ –æ–¥–µ–∂–¥—ã** –ú–∞—Ä–∏–æ. –¢–∞–∫ –∂–µ –≤–∏–¥–Ω–æ —á—Ç–æ –≤—Å–µ –∫–∞–Ω–∞–ª—ã –º–∞–ª–æ —Å–ø–æ—Å–æ–±—Å—Ç–≤—É—é—Ç —É—Å–∞–º, —Å–∞–º—ã–º —Ç–µ–º–Ω—ã–º —á–∞—Å—Ç—è–º –∫–∞—Ä—Ç–∏–Ω–∫–∏.

–£ –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–∞–ª–∞ –æ–ø—Ä–µ–¥–∏–ª–µ–Ω–Ω–æ–µ –∫–∞–ª–∏—á–µ—Å—Ç–≤–æ –±–∏—Ç–æ–≤, —Ç–∞–∫ –Ω–∞–∑—ã–≤–∞–µ–º–∞ **–±–∏—Ç–æ–≤–∞—è –≥–ª—É–±–∏–Ω–∞**. –ï—Å–ª–∏ —É –Ω–∞—Å **8 –±–∏—Ç–æ–≤** (–º–µ–∂–¥—É 0 –∏ 255–∏) –¥–ª—è –∫–∞–∂–¥–µ–≥–æ —Ü–≤–µ—Ç–∞, —Ç–æ –±–∏—Ç–æ–≤–∞—è –≥–ª—É–±–∏–Ω–∞ —É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π RGB (8 –±–∏—Ç * 3 —Ü–≤–µ—Ç–∞) –≤ —Ä–∞–∑–º–µ—Ä–µ **24 –±–∏—Ç–∞**, —Ç–æ –µ—Å—Ç—å 2^24 —Ä–∞–∑–Ω—ã—Ö —Ü–≤–µ—Ç–∞.

> **–•–æ—Ä–æ—à–æ –∑–Ω–∞—Ç—å** [–∫–∞–∫ –æ–∑–ø–±—Ä–∞–∂–µ–Ω–∏—è –ø–µ—Ä–µ–≤–æ–¥—è—Ç—Å—è —Å –Ω–∞—Å—Ç–æ—è—â–µ–≥–æ –º–∏—Ä–∞ –≤ –±–∏—Ç—ã](http://www.cambridgeincolour.com/tutorials/camera-sensors.htm).

–î—Ä—É–≥–∞—è —á–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –µ—Ç–æ **—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ**, —Ç–æ –µ—Å—Ç—å –∫–∞–ª–∏—á–µ—Å—Ç–≤–æ –ø–∏–∫—Å–µ–ª–µ–π –≤ –æ–¥–Ω–æ–º –∏–∑–º–µ—Ä–µ–Ω–∏–µ. –û–±—ã—á–Ω–æ –æ–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—å—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ —à—ã—Ä–∏–Ω–∞ —Ö –≤—ã—Å–æ—Ç–∞, –∫–∞–∫ –Ω–∞ **4—Ö4** –∫–∞—Ä—Ç–∏–Ω–∫–µ —Å–Ω–∏–∑—É.

![–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è](/i/resolution.png "–†–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")

> #### –ü—Ä–∞–∫—Ç–∏–∫–∞: –µ–∫—Å–ø–µ—Ä–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º —Ü–≤–µ—Ç–∞–º–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏
> –ú–æ–∂–Ω–æ [–∏–≥—Ä–∞—Ü–∞ —Å —Ü–≤–µ—Ç–∞–º–∏ –≤ –∫–∞—Ä—Ç–∏–Ω–∫–∞—Ö](/image_as_3d_array.ipynb) –∏—Å–ø–æ–ª—å–∑—É—è [jupyter](#how-to-use-jupyter) (python, numpy, matplotlib –∏ —Ç.–¥).
>
> –¢–∞–∫ –∂–µ –º–æ–∂—à–Ω–æ –ø–æ–Ω—è—Ç—å [–∫–∞–∫ —Ñ–∏–ª—å—Ç—Ä—ã, (–æ–±–æ—Å—Ç—Ä–µ–Ω–∏–µ, —Ä–∞–∑–º—ã—Ç–æ—Å—Ç—å...) —Ä–∞–±–æ—Ç–∞—é—Ç](/filters_are_easy.ipynb).

–ï—â–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å –∫–æ—Ç–æ—Ä—ã–º –º—ã –≤—Å—Ç—Ä–µ—á–∞–µ–º—Å—è –ø—Ä–µ —Ä–∞–±–æ—Ç–µ —Å –≤–∏–¥–µ–æ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —ç—Ç–æ **—Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω (AR)** –∫–æ—Ç–æ—Ä–æ–µ –ø—Ä–æ—Å—Ç–æ –æ–ø–∏—Å–æ–≤–∞–µ—Ç –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ —à–∏—Ä–∏–Ω—ã —Å—Ä–∞–Ω–≤–Ω–∏—Ç–µ–ª—å–Ω–æ —Å –≤—ã—Å–æ—Ç–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –ø–∏–∫—Å–µ–ª—è.

–ö–æ–≥–¥–∞ –ª—É–¥–∏ –≥–æ–≤–æ—Ä—è—Ç —á—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–ª–∏ —Ñ–∏–ª—ä–º **16—Ö9**, –æ–Ω–∏ –Ω–∞–∑—ã–≤–∞—é—Ç —Ü–∏—Ñ—Ä—ã **DAR (–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω –¥–∏—Å–ø–ª–µ—è)**. –¢–∞–∫ –∂–µ –∏ –±—ã–≤–∞–µ—Ç **PAR (–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Å—Ç–æ—Ä–æ–Ω –ø–∏–∫—Å–µ–ª—è)**

![DAR](/i/DAR.png "DAR")

![PAR](/i/PAR.png "PAR")

> #### DVD —ç—Ç–æ DAR 4:3
> 
> –•–æ—Ç—å —É DVD —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ 704—Ö480, —Ñ–æ—Ä–º–∞—Ç –≤—Å–µ —Ä–∞–≤–Ω–æ —Å–æ–±–ª—é–¥–∞–µ—Ç DAR 4:3 –ø–æ—Ç–æ–º—É —á—Ç–æ —É –Ω–µ–≥–æ PAR 10:11 (703x10/480x11)

–ò –∫–æ–Ω–µ—á–Ω–æ, "–≤–∏–¥–µ–æ" –º—ã –æ–ø—Ä–µ–¥–∏–ª—è–µ–º –∫–∞–∫ **–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–µ *n*-–∫–∞–¥—Ä–æ–≤** –≤–æ **–≤—Ä–µ–º–∏–Ω–∏**, —á—Ç–æ –º–æ–∂–Ω–æ –æ–±–æ–∑–Ω–∞—á–∏—Ç—å –∫–∞–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–º–µ—Ä–µ–Ω–∏–µ –ø–æ–≤–µ—Ä—Ö—É —Ä–∞–∑–º–µ—Ä–∞ –∏ —Ü–≤–µ—Ç–∞, –≥–¥–µ *–ø* —ç—Ç–æ –∫–æ–ª—É—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É (FPS).

![–≤–∏–¥–µ–æ](/i/video.png "–≤–∏–¥–µ–æ")

**–±–∏—Ç—Ä–µ–π—Ç** —ç—Ç–æ —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ –±–∏—Ç–æ–≤ —á—Ç–æ –±—ã –ø–æ–∫–∞–∑–∞—Ç—å —Å–µ–∫—É–Ω–¥—É –≤–∏–¥–µ–æ.

> –±–∏—Ç—Ä–µ–π—Ç = —à–∏—Ä–∏–Ω–∞ * –≤—ã—Å–æ—Ç–∞ * –±–∏—Ç–æ–≤–∞—è –≥–ª—É–±–∏–Ω–∞ * –∫–∞–¥—Ä—ã –≤ —Å–µ–∫—É–Ω–¥—É

–ù–∞–ø—Ä–∏–º–µ—Ä, –≤–∏–¥–µ–æ –∫–æ—Ä–æ–µ 30 FPS, 24 –±–∏—Ç–∞ –Ω–∞ –ø–∏–∫—Å–µ–ª, —Å —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º 480—Ö240 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **82,944,000 –±–∏—Ç–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É** –∏–ª–∏ 82 –º–µ–≥–∞–±–∏—Ç–∞/—Å (30*480—Ö240—Ö24) –±–µ–∑ –∫–æ–º–ø—Ä–µ—Å—Å–∏–∏/—Å–Ω–∏–∂–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞.

–ö–æ–≥–¥–∞ **–±–∏—Ç —Ä–µ–π—Ç** –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π —Ç–æ –µ–≥–æ –Ω–∞–∑—ã–≤–∞—é—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –±–∏—Ç—Ä–µ–π—Ç–æ–º (**CBR**), –∫–æ–≥–¥–∞ –æ–Ω –º–µ–Ω—è–µ—Ç—Å—å—è —Å–æ –≤—Ä–µ–º–∏–Ω–µ–º —Ç–æ–≥–¥–∞ —ç—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–π –±–∏—Ç—Ä–µ–π—Ç (**VBR**).

> –≠—Ç–æ—Ç –≥—Ä–∞—Ñ–∏–∫ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π VBR, –∫–æ—Ç–æ—Ä—ã–π –Ω–µ —Ç—Ä–∞—Ç–∏—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –±–∏—Ç–æ–≤ –Ω–∞ —á–µ—Ä–Ω—ã–µ –∫–∞–¥—Ä—ã.
>
> ![–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π VBR](/i/vbr.png "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π VBR")

–†–∞–Ω—å—à–µ, –µ–Ω–∂–∏–Ω–µ—Ä—ã –ø—Ä–µ–¥—É–º–∞–ª–∏ —Ç–µ—Ö–Ω–∏–∫—É —à—Ç–æ –±—ã —É–≤–µ–ª–∏—á–∏—Ç—å –≤–æ—Å–ø—Ä–∏–Ω—è—Ç—ã–π FPS –≤ –¥–≤–∞ —Ä–∞–∑–∞ –ø—Ä–∏ —ç—Ç–æ–º –Ω–µ **–Ω–µ —É–≤–µ–ª–∏—á–µ–≤—ã—è –±–∏—Ç—Ä–µ–π—Ç**. –≠—Ç–∞ —Ç–µ—Ö–Ω–∏–∫–∞ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—å—è **—á–µ—Ä–µ—Å—Å—Ç—Ä–æ—á–Ω–æ–µ –≤–∏–¥–µ–æ**, –≥–¥–µ –ø–æ–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ—Å—ã–ª–∞–µ—Ç—Å—å—è –≤ –æ–¥–Ω–æ–º "–∫–∞–¥—Ä–µ" –∏ –¥—Ä—É–≥–∞—è –ø–æ–ª–æ–≤–∏–Ω–∞ –≤ —Å–ª–µ–¥—É—â–µ–º "–∫–∞–¥—Ä–µ".

–ù–∞ —Å–µ–≤–æ–¥–Ω–µ—à–Ω–∏–π –¥–µ–Ω—å —ç–∫—Ä–∞–Ω—ã –æ–±—ã—á–Ω–æ –ø—Ä–µ–¥—Å—Ç–æ–≤–ª—è—é—Ç –≤–∏–¥–µ–æ —Ç–µ—Ö–Ω–∏–∫–æ–π **–ø—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω–æ–≥–æ —Å–∫–∞–Ω–∞**. –≠—Ç–æ —Å–ø–æ—Å–æ–± –ø–æ–∫–∞–∑—ã–≤–∞–Ω–∏—è, —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–µ—Ä–µ–≤–æ–¥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –∫–æ—Ç–æ—Ä–æ–º –≤—Å–µ –ª–∏–Ω–∏–∏ –∫–∞–∂–¥–µ–≥–æ –∫–∞–¥—Ä–∞ –Ω–∞—Ä–∏—Å–æ–≤–∞–Ω—ã –æ–¥–Ω–∞ –∑–∞ –¥—Ä—É–≥–æ–π.

![—á–µ—Ä–µ—Å—Å—Ç—Ä–æ—á–Ω–æ–µ –≤–∏–¥–µ–æ –ø—Ä–æ—Ç–∏–≤ –ø—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω—ã–π —Å–∫–∞–Ω](/i/interlaced_vs_progressive.png "—á–µ—Ä–µ—Å—Å—Ç—Ä–æ—á–Ω–æ–µ –≤–∏–¥–µ–æ –ø—Ä–æ—Ç–∏–≤ –ø—Ä–æ–≥—Ä–µ—Å–∏–≤–Ω—ã–π —Å–∫–∞–Ω")

–¢–µ–ø–µ—Ä—å —É –Ω–∞—Å –µ—Å—Ç—å –ø—Ä–µ–¥–∏—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–≥–æ –∫–∞–∫ x—Ä–∞–Ω–∏—Ü–∞ **–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ** –≤ —Ü–∏—Ñ—Ä–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ, –∫–∞–∫ –µ–≥–æ **—Ü–≤–µ—Ç–∞** —Ä–∞–∑–ø–æ–ª–æ–∂–µ–Ω—ã, —Å–∫–æ–ª—å–∫–æ **–±–∏—Ç–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É** –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —á—Ç–æ –±—ã –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –≤–∏–¥–µ–æ, –µ—Å–ª–∏ —ç—Ç–æ –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π –±–∏—Ç—Ä–µ–π—Ç (CBR) –∏–ª–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–π –±–∏—Ç—Ä–µ–π—Ç (VBR), –∏ –∫–∞–∫ —ç—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å **—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º** –ø—Ä–∏ –¥–∞–Ω–Ω–æ–π **—á–∏—Å—Ç–æ—Ç–µ –∫–∞–¥—Ä–∞**. –¢–∞–∫ –∂–µ –º—ã –±–æ–ª–µ–µ –∑–Ω–∞–∫–æ–º—ã —Å —Ç–µ—Ä–º–∏–Ω–∞–º–∏ –Ω–∞ –ø–æ–¥–æ–±–∏–µ **—á–µ—Ä–µ—Å—Å—Ç—Ä–æ—á–Ω–æ–µ –≤–∏–¥–µ–æ**, PAR, –∏ —Ç.–¥. 

> #### –ü—Ä–∞–∫—Ç–∏–∫–∞: –†–∞—Å—Å–º–æ—Ç—Ä —Å–≤–æ–π—Å–≤—Ç–≤–∞ –≤–∏–¥–µ–æ
> –í–∞–º –≤–æ–∑–º–æ–∂–Ω–æ [—É–≤–∏–¥–µ—Ç—å –æ–≥–æ–≤–æ—Ä–∏–Ω–Ω—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞ —Å –ø–æ–º–æ—â—å—é ffmpeg –∏–ª–∏ mediainfo.](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#inspect-stream)

# –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏

–ë—ã—Å—Ç—Ä–æ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—å—è —è—á–µ–≤–∏–¥–Ω–æ —á—Ç–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –≤–∏–¥–µ–æ –±–µ–∑ –∫–æ–º–ø—Ä–µ—Å–∏–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–∏ –Ω–µ –≤–æ–∑–º–æ–∂–Ω–æ; **–æ–¥–∏–Ω —á–∞—Å –≤–∏–¥–µ–æ** –ø—Ä–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ 720p, —Å —á–∞—Å—Ç–æ—Ç–æ–π –∫–∞–¥—Ä–∞ 30fps –∑–∞–Ω–∏–º–∞–µ—Ç **278GB –ø–∞–º—è—Ç–∏<sup>*</sup>**. –ê–ª–≥–æ—Ä–∏—Ç–º—ã –∫–æ–º–ø—Ä–µ—Å–∏–∏ –±–µ–∑ –ø–æ—Ç–µ—Ä—å, –Ω–∞ –ø–æ–¥–æ–±–∏–µ DEFLATE (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–µ—Ç—Å—è –≤ PKZIP, Gzip, PNG) **–Ω–µ —Å–∂–∏–º–∞—é—Ç –≤–∏–¥–µ–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ** –Ω–∞–º –Ω–∞–¥–æ –∏—Å–∫–∞—Ç—å –¥—Ä—É–≥–∏–µ —Å–ø–æ—Å–æ–±—ã —Å–∂–∞—Ç—å—è –≤–∏–¥–µ–æ.

> <sup>*</sup> –≠—Ç—É —Ü–∏—Ñ—Ä—É –º—ã –ø–æ–ª—É—á–∞–µ–º —É–º–Ω–∞–∂–∞—è 1280 —Ö 720 —Ö 24 —Ö 30 —Ö 3600 (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞, –≤–∏—Ç–æ–≤–∞—è –≥–ª—É–±–∏–Ω–∞, —á–∏—Å—Ç–æ—Ç–∞ –∫–∞–¥—Ä–∞ –∏ –≤—Ä–µ–º—è –≤ —Å–µ–∫—É–Ω–¥–∞—Ö).

–î–ª—è —ç—Ç–æ–≥–æ –º—ã –º–æ–∂–µ–º –µ–∫—Å–ø–ª–æ–π—Ç–µ—Ä–∏–≤–æ–∞—Ç—å **—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –Ω–∞—à–µ–≥–æ –∑—Ä–µ–Ω–∏—è**. –ú—ã —Ä–∞–∑–±–µ—Ä–∞–µ–º —è—Ä–∫–æ—Å—Ç—å –ª—É—á—à–µ —á–µ–º —Ü–≤–µ—Ç–∞. –¢–∞–∫ –∂–µ –±–æ–ª–µ–µ –∑–∞–º–µ—Ç–Ω—ã–µ **–ø–æ–≤—Ç–æ—Ä—ã —á–∞—Å—Ç–µ–π –∏–∑–æ–±—Ä–∞–∑–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º–∏–Ω–∏**.

## –¶–≤–µ—Ç–∞, —è—Ä–∫–æ—Å—Ç—å –∏ –≥–ª–∞–∑–∞

–ù–∞—à–∏ –≥–ª–∞–∑–∞ –±–æ–ª–µ–µ —á—É—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∫ [—è—Ä–∫–æ—Å—Ç–µ —á–µ–º –∫ —Ü–≤–µ—Ç–∞–º](http://vanseodesign.com/web-design/color-luminance/), –ø—Ä–æ–≤–µ—Ä—Ç–∏ –¥–ª—è —Å–µ–±—è –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ:

![—è—Ä–∫–æ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤ —Ü–≤–µ—Ç–∞](/i/luminance_vs_color.png "—è—Ä–∫–æ—Å—Ç—å –ø—Ä–æ—Ç–∏–≤ —Ü–≤–µ—Ç–∞")

–ï—Å–ª–∏ –≤—ã –Ω–µ –≤–∏–¥–µ—Ç–∏ —á—Ç–æ —Ü–≤–µ—Ç–∞ **–∫–≤–∞–¥—Ä–∞—Ç–∞ –ê –∏ –ë –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ** —É –∫–∞—Ä—Ç–∏–Ω–∫–µ —Å –ª–µ–≤–∞, —Ç–æ —É –≤–∞—Å –≤—Å–µ –≤ –ø–æ—Ä—è–¥–∫–µ, –Ω–∞—à –º–æ–∑–≥ –Ω–∞—Å—Ç—Ä–æ–∏–Ω **—É–¥–µ–ª—è—Ç—å –±–æ–ª—å—à–µ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —è—Ä–∫–æ—Å—Ç—å/—Ç–µ–º–Ω–æ—Ç—É —á–µ–º –Ω–∞ —Ü–≤–µ—Ç**. –° –ø—Ä–∞–≤–æ –≤—ã –≤–∏–¥–∏—Ç–µ —à—Ç–æ —Ü–≤–µ—Ç–∞ –∏ –≤ –ø—Ä–∞–≤–¥—É –æ–¥–∏–Ω–∞–∫–æ–≤—ã.

> **–£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ–±—å—è—Å–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –≥–ª–∞–∑**
> –ì–ª–∞–∑ [—Å–ª–æ–∂–Ω—ã–π –æ—Ä–≥–∞–Ω](http://www.biologymad.com/nervoussystem/eyenotes.htm), —Å–æ—Å—Ç–∞–≤–ª–∏–Ω –∏–∑ –º–Ω–æ–≥–∏—Ö —á–∞—Å—Ç–µ–π, —Ö–æ—Ç—è –Ω–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç —à–∏—à–æ—á–Ω—ã–µ –∏ –ø–∞–ª–æ—á–Ω—ã–µ –∫–ª–µ—Ç–∫–∏. –ì–ª–∞–∑ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –ø–æ—Ä—è–¥–∫–∞ [120 –º–∏–ª–∏–æ–Ω –ø–∞–ª–æ—á–Ω—ã—Ö –∫–ª–µ—Ç–æ–∫ –∏ 6 –º–∏–ª–∏–æ–Ω —à–∏—à–æ—á–Ω—ã—á –∫–ª–µ—Ç–æ–∫](https://en.wikipedia.org/wiki/Photoreceptor_cell).
>
> **–£–ø—Ä–∞—â–∞—è –¥–∞–ª—å—à–µ**, –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫ —Ü–≤–µ—Ç –∏ —è—Ä–∫–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤—É—é—Ç –Ω–∞ –≥–ª–∞–∑–∞. **[–ü–∞–ª–æ—á–Ω—ã–µ –∫–ª–µ—Ç–∫–∏](https://en.wikipedia.org/wiki/Rod_cell) –ø–æ –±–æ–ª—å—à–µ–Ω—Å—Ç–≤—É –æ—Ç–≤–µ—Ü—Ç–≤–µ–Ω–Ω—ã –∑–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —è—Ä–∫–æ—Å—Ç–∏**. **[–®–∏—à–∏—á–Ω—ã–µ –∫–ª–µ—Ç–∫–∏](https://en.wikipedia.org/wiki/Cone_cell) –æ—Ç–≤–µ—Ü—Ç–≤–µ–Ω–Ω—ã –∑–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —Ü–≤–µ—Ç–∞**. –ï—Å—Ç—ã —Ç—Ä–∏ —Ç–∏–ø–∞ —à–∏—à–∫–∏, –∫–∞–∂–¥–∞—è —Å–æ —Å–≤–æ–µ–π –æ–∫—Ä–∞—Å–∫–æ–π: [S-—à–∏—à–∫–∏ (–°–∏–Ω–∏–π), M-—à–∏—à–∫–∏ (–ó–µ–ª–µ–Ω—ã–π) –∏ L-cones (–ö—Ä–∞—Å–Ω—ã–π)](https://upload.wikimedia.org/wikipedia/commons/1/1e/Cones_SMJ2_E.svg).
>
> –ü–æ—Ç–æ–º—É —á—Ç–æ —É –Ω–∞—Å –±–æ–ª—å—à–µ –ø–∞–ª–æ—á–Ω—ã—Ö –∫–ª–µ—Ç–æ–∫ (—è—Ä–∫–æ—Å—Ç—å) —á–µ–º —à–∏—à–æ—á–Ω—ã—Ö (—Ü–≤–µ—Ç), –º—ã –¥–µ–ª–∞–µ–º –≤—ã–≤–æ–¥ —á—Ç–æ –º—ã –æ–±—Ä–æ–±–∞—Ç—ã–≤–∞–µ–º –±–æ–ª—å—à–µ —è—Ä–∫–æ—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.
>
> ![—Å–æ—Å—Ç–∞–≤ –≥–ª–∞–∑](/i/eyes.jpg "—Å–æ—Å—Ç–∞–≤ –≥–ª–∞–∑")
>
> **–§—É–Ω–∫—Ü–∏–∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**
> 
> –ï—Å—Ç—å –º–Ω–æ–≥–æ —Ç–µ–æ—Ä–∏–π –æ–ø–∏—Å—ã–≤–∞—è —Ñ—É–Ω–∫—Ü–∏–∏ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –∑—Ä–µ–Ω–∏–µ. –û–¥–Ω–∞ –∏–∑ –Ω–∏—Ö –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è "–§—É–Ω–∫—Ü–∏–∏ –∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏". –û–Ω–∏ –æ–ø–∏—Å—ã–≤–∞—é—Ç —Å–∫–æ–ª—å–∫–æ –∏–∑–º–∏–Ω–µ–Ω–∏—è –≤ —Ü–≤–µ—Ç–µ –º–æ–∂–µ—Ç –ø—Ä–æ–∏–∑–æ–π—Ç–∏ –ø–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ –Ω–∞–±–ª—é–¥–∞—é—â–∏–π –µ–≥–æ –∑–∞–º–µ—á–∞–µ—Ç. –§—É–Ω–∫—Ü–∏–∏ –∏–∑–º–µ—Ä—è—é—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —á—É—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ —á–µ—Ä–Ω–æ –±–µ–ª–æ–º—É, –Ωo —Ç–∞–∫ –∂–µ –∏ —è—Ä–∫–æ—Å—Ç—å –ø—Ä–∏ —Ü–≤–µ—Ç–∞—Ö. –ï–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Ä–∞–∑ –∑–∞ —Ä–∞–∑–æ–º –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —á—Ç–æ –º—ã –±–æ–ª–µ–µ —á—É—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∫ —è—Ä–∫–æ—Å—Ç–∏ —á–µ–º –∫ —Ü–≤–µ—Ç–∞–º.

–ó–Ω–∞—è —á—Ç–æ –º—ã —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã –∫ —è—Ä–∫–æ—Å—Ç–µ, –º—ã –º–æ–∂–µ–º –ø–æ–ø—ã—Ç–∞—Ç—Å—è —ç–∫—Å–ø–ª–æ–π—Ç–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç —Ñ–∞–∫—Ç.

### –ú–æ–¥–µ–ª—å —Ü–≤–µ—Ç–∞

–í –Ω–∞—á–∞–ª–µ –º—ã –∏–∑—É—á–∞–ª–∏ –∫–∞–∫ [—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ü–≤–µ—Ç –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö](#–æ—Å–Ω–æ–≤–Ω—ã–µ-—Ç–µ—Ä–º–∏–Ω—ã) –∏—Å–ø–æ–ª—å–∑—É—è **–º–æ–¥–µ–ª—å RGB**. –°—É—â–µ—Å—Ç–≤—É—é—Ç –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ - –Ω–∞–ø—Ä–∏–º–µ—Ä, **YCbCr**<sup>*</sup> –¥–µ–ª–∏—Ç luma/–ª—É–º–∞ (—è—Ä–∫–æ—Å—Ç—å) –æ—Ç chrominance/—Ö—Ä–æ–º–∏–Ω–∞–Ω—Ü (—Ü–≤–µ—Ç–Ω–æ—Å—Ç—å).

> <sup>*</sup> –µ—Å—Ç—å –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –∫–æ—Ç–æ—Ä—ã–µ —Ç–∞–∫ –∂–µ —Ä–∞–∑–¥–µ–ª—è—é—Ç –º–µ–∂–¥—É —è—Ä–∫–æ—Å—Ç—å—é –∏ —Ü–≤–µ—Ç–æ–º.

–í —ç—Ç–æ–π –º–æ–¥–µ–ª–µ **Y** –∫–∞–Ω–∞–ª —è—Ä–∫–æ—Å—Ç–∏. –¢–∞–∫ –∂–µ –µ—Å—Ç—å –¥–≤–∞ –∫–∞–Ω–∞–ª–∞ —Ü–≤–µ—Ç–∞, **Cb** (—Å–∏–Ω—è—è chroma/—Ö—Ä–æ–º–∞) –∏ **Cr** (–∫—Ä–∞—Å–Ω–∞—è chroma/—Ö—Ä–æ–º–∞). –§–æ—Ä–º–∞—Ç [YCbCr](https://en.wikipedia.org/wiki/YCbCr) –≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –æ—Ç RGB, –∏ —Ç–∞–∫ –∂–µ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å RGB –æ—Ç YCbCr. –° –ø–æ–º–æ—â—å—é YCbCr –º—ã –º–æ–∂–µ–º —Å–æ—Å—Ç–æ–≤–ª—è—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–æ–ª–Ω–æ–º —Ü–≤–µ—Ç–µ:

![–ø—Ä–∏–º–µ—Ä ycbcr](/i/ycbcr.png "–ø—Ä–∏–º–µ—Ä ycbcr")

### Converting between YCbCr and RGB

–ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –≤–∞—Å –º–æ–∂–µ—Ç —Å–ø—Ä–∞—à–∏–≤–∞—é—Ç, –∫–∞–∫ –º–æ–∂–Ω–æ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å **–≤—Å–µ —Ü–≤–µ—Ç–∞ –±–µ–∑ –∑–µ–ª–µ–Ω–æ–≥–æ**?

–ß—Ç–æ –±—ã –ø–æ–Ω—è—Ç–∂—å –æ—Ç–≤–µ—Ç, –¥–∞–≤–∞–π—Ç–µ –ø–æ—Å–º–æ—Ç—Ä–∏–º –∫–∞–∫ –º—ã –ø–µ—Ä–µ–≤–æ–¥–∏–º —Ü–≤–µ—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å RGB –Ω–∞ YCbCr. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–æ—ç—Ñ–∏—Ü–µ–Ω—Ç—ã –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ **[BT.601](https://en.wikipedia.org/wiki/Rec._601)**, –∫–æ—Ç–æ—Ä—ã–π –±—ã–ª —Å–æ–∑–¥–∞–Ω **[–≥—Ä—É–ø–ø–æ–π ITU-R<sup>*</sup>](https://en.wikipedia.org/wiki/ITU-R)**. –ü–µ—Ä–≤—ã–π —à–∞–≥ –≤ —ç—Ç–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ —ç—Ç–æ **–≤—ã—Å—á–∏—Ç–∞—Ç—å —è—Ä–∫–æ—Å—Ç—å (luma)** –∏ –∑–∞–º–µ–Ω–∏—Ç—å —Ü—ã—Ñ—Ä—ã RGB, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã —Ä–∞–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã –≥—Ä—É–ø–ø–æ–π ITU.

```
Y = 0.299R + 0.587G + 0.114B
```

–ü–æ—Å–ª–µ —è—Ä–∫–æ—Å—Ç–∏, –º—ã –º–æ–∂–µ–º **–ø–æ–ª—É—á–∏—Ç—å —Ü–≤–µ—Ç–∞** —Ü–≤–µ—Ç–∞ (chroma —Å–∏–Ω–∏–π –∏ –∫—Ä–∞—Å–Ω—ã–π):

```
Cb = 0.564(B - Y)
Cr = 0.713(R - Y)
```

–ú–æ–∂–Ω–æ **–ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —ç—Ç–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—Ä–∞—Ç–Ω–æ**, –∏ –¥–∞–∂—à–µ **–ø–æ–ª—É—á–∏—Ç—å –∑–µ–ª–µ–Ω—ã–π –∏—Å–ø–æ–ª—å–∑—É—è YCbCr**

```
R = Y + 1.402Cr
B = Y + 1.772Cb
G = Y - 0.344Cb - 0.714Cr
```

> <sup>*</sup> –¶—ã—Ñ—Ä–æ–≤—ã–º –≤–∏–¥–µ–æ –ø—Ä–∞–≤—è—Ç –≥—Ä—É–ø–ø—ã –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã. –ì—Ä—É–ø–ø—ã –æ–ø—Ä–µ–¥–µ–ª—è—é—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã, –Ω–∞ –ø—Ä–∏–º–µ—Ä [—á—Ç–æ —Ç–∞–∫–æ–µ 4–ö? –ö–∞–∫—É—é —á–∏—Å—Ç–æ—Ç—É –∫–∞–¥—Ä–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—ä? –†–µ—à–µ–Ω–∏–µ? –¶–≤–µ—Ç–Ω—É—é –º–æ–¥–µ–ª—å?](https://en.wikipedia.org/wiki/Rec._2020).

–û–±—ã—á–Ω–æ, **–¥–∏—Å–ø–ª–µ–∏** —Ä–∞–±–æ—Ç–∞—é—Ç —Ç–æ–ª—å–∫–æ –≤ —Ä–µ–∂–∏–º–µ –º–æ–¥–µ–ª–∏ **RGB**, —Ä–∞–∑–ø–æ–ª–æ–∂–µ–Ω—ã–µ –∫–∞–∫–∏–º —Ç–æ –æ–±—Ä–∞–∑–æ–º. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –∏–∑ –Ω–∏—Ö –ø–æ–∫–∞–∑–∞–Ω–Ω—ã —Ç—É—Ç:

![–ø–∏–∫—Å–µ–ª—å–Ω–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è](/i/new_pixel_geometry.jpg "–ø–∏–∫—Å–µ–ª—å–Ω–∞—è –≥–µ–æ–º–µ—Ç—Ä–∏—è")

### –¶–≤–µ—Ç–æ–≤–∞—è —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è

–ö–æ–≥–¥–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏ —è—Ä–∫–æ—Å—Ç–∏ –∏ —Ü–≤–µ—Ç–∞, –º—ã –º–æ–∂—à–µ–º –≤–æ—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—Å—è —á—É—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –≥–ª–∞–∑–∞ –∫ —è—Ä–∫–æ—Å—Ç–µ —á—Ç–æ –±—ã —É–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. **–¶–≤–µ—Ç–æ–≤–∞—è —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è** —Ç–µ—Ö–Ω–∏–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω—å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏ —Å **—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ–º –¥–ª—è —Ü–≤–µ—Ç–∞ –º–µ–Ω—å—à–µ —á–µ–º –¥–ª—è —è—Ä–∫–æ—Å—Ç–∏**.

![—Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è ycbcr](/i/ycbcr_subsampling_resolution.png "—Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è ycbcr")

–¢–∞–∫ –∫–∞–∫ –∂–µ —É–º–µ–Ω—à–∞–µ—Ç—å—Å—è —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞?! –°—É—â–µ—Å—Ç–≤—É—é—Ç —Å—Ö–µ–º—ã –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–∏—Å–æ–≤–∞—é—Ç –∫–∞–∫ –ø–æ–ª—É—á–∞—Ç—å —Ü–≤–µ—Ç –æ—Ç —Ä–∞–∑–Ω—ã—Ö —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–π –¥–ª—è –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (`–∫–æ–Ω–µ—á–Ω—ã–π —Ü–≤–µ—Ç = Y + Cb + Cr`).

–≠—Ç–∏ —Å—Ö–µ–º—ã –Ω–∞–∑—ã–≤–∞—é—Ç—Å—è —Å–∏—Å—Ç–µ–º–∞–º–∏ —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏, –æ–±—ã—á–Ω–æ –ø–æ–∫–∞–∑–∞–Ω—ã —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ–º —Ç—Ä–µ—Ö —á–∏—Å–µ–ª - `a:x:y` –∫–æ—Ç–æ—Ä–æ–µ –æ–ø—Ä–µ–¥–∏–ª—è–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ —Ü–≤–µ—Ç–∞ –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ `a * 2` –±–ª–æ–∫—É –ø–∏–∫—Å–µ–ª–µ–π —è—Ä–∫–æ—Å—Ç–∏.

 * `a` –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω–∞—è —à–∏—Ä–∏–Ω–∞ (–æ–±—ã—á–Ω–æ 4)
 * `x` –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤ —Ü–≤–µ—Ç–∞ –≤ –ø–µ—Ä–≤–æ–º —Ä—è–¥—É
 * `y` –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π —Ü–≤–µ—Ç–∞ –º–µ–∂–¥—É –ø–µ—Ä–≤—ã–º –∏ –≤—Ç–æ—Ä—ã–º —Ä—è–¥–æ–º
 
> –í —ç—Ç–æ–π —Å—Ö–µ–º–µ –µ—Å—Ç—å –∏–∑–∫–ª—É—á–µ–Ω–∏–µ, 4:1:0, –≥–¥–µ –≤—ã–±–µ—Ä–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω —Ü–≤–µ—Ç –¥–ª—è –∫–∞–∂–¥–µ–≥–æ `4 x 4` –±–ª–æ–∫–∞ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —è—Ä–∫–æ—Å—Ç–∏. 

–í —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∫–æ–¥–µ–∫–∞—Ö –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—ã—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—ã: **4:4:4** *(–ë–µ–∑ —Å—É–±–¥–∏—Å–∫—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏)*, **4:2:2, 4:1:1, 4:2:0, 4:1:0 –∏ 3:1:1**.

> –í—ã –º–æ–∂–µ—Ç–µ —É—á–∞—Å—Ç–æ–≤–∞—Ç—å –≤ [—Ä–∞–∑–≥–æ–≤–æ—Ä–∞—Ö –æ —Ü–≤–µ—Ç–æ–≤–æ–π –ø–æ–¥–±–æ—Ä–∫–µ —Ç—É—Ç](https://github.com/leandromoreira/digital_video_introduction/issues?q=YCbCr).

> **–ü–æ–¥–±–æ—Ä–∫–∞ YCbCr 4:2:0**
>
> –¢—É—Ç –≤–∏–¥–Ω–æ –∫–∞–∫ —Å–ª–∏–≤–∞–µ—Ç—Å—è —Ü–≤–µ—Ç–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å —è—Ä–∫–æ—Å—Ç–Ω–æ–π –≤ —Ä–µ–∂—à–∏–º–µ YCbCr 4:2:0. –ó–∞–º–µ—Ç–∏—Ç–µ, —á—Ç–æ –º—ã —Ç–æ–ª—å–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º 12 –±–∏—Ç–æ–≤ –Ω–∞ –ø–∏–∫—Å–µ–ª—å.
>
> ![—Å–ª–∏—Ç–∏–µ YCbCr 4:2:0](/i/ycbcr_420_merge.png "—Å–ª–∏—Ç–∏–µ YCbCr 4:2:0 merge")

–¢—É—Ç –ø–æ–∫–∞–∑–æ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–æ–µ –≥–ª–∞–≤–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è. –ü–µ—Ä–≤—ã–π —Ä—è–¥ —Ñ–∏–Ω–∞–ª—å–Ω–∏–π YCbCr; –Ω–∞ –≤—Ç–æ—Ä–æ–º, –≤–∏–¥–Ω–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ —Ü–≤–µ—Ç–Ω—ã—Ö –∫–∞–Ω–∞–ª–æ–≤. –ö–∞—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ, –ø—Ä–∏ —ç—Ç–æ–º –ø–∞–º—è—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ –º–Ω–æ–≥–æ –º–µ–Ω—å—à–µ.

![–ø—Ä–∏–º–µ—Ä—ã —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è](/i/chroma_subsampling_examples.jpg "–ø—Ä–∏–º–µ—Ä—ã —Å—É–±–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è")

–†–∞–Ω—å–µ–µ –º—ã –≤—ã—á–µ—Å–ª–∏–ª–∏ —á—Ç–æ –Ω–∞–º –Ω–∞–¥–æ [278GB –ø–∞–º—è—Ç–∏ —á—Ç–æ –±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–∞—Å 720p/30fps –≤–∏–¥–µ–æ](#—É–¥–∞–ª–µ–Ω–∏–µ-–∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏). –ò—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∂–∏–º **YCbCr 4:2:0** –º—ã –º–æ–∂–µ–º **—É–±—Ä–∞—Ç—å –ø–æ–ª–æ–≤–∏–Ω—É —Ä–∞–∑–º–µ—Ä–∞ –¥–æ 139GB**<sup>*</sup>, —Ö–æ—Ç—è —ç—Ç–æ –≤—Å–µ —Ä–∞–≤–Ω–æ –¥–∞–ª–µ–∫–æ –æ—Ç –∏–¥–µ–∞–ª–∞.

> <sup>*</sup> —ç—Ç–æ —á–∏—Å–ª–æ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å —É–º–Ω–∞–∂–∞—è —à–∏—Ä–∏–Ω—É, –≤—ã—Å–æ—Ç—É, –±–∏—Ç—ã –Ω–∞ –ø–∏–∫—Å–µ–ª, –∏ —á–∏—Å—Ç–æ—Ç—É –∫–∞–¥—Ä–∞. –†–∞–Ω—å—à–µ –º—ã –ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å 24 –±–∏—Ç–∞–º–∏, —Ç–µ–ø–µ—Ä—å –Ω–∞–º —Ç–æ–ª—å–∫–æ –Ω–∞–¥–æ 12.

<br/>

> ### Hands-on: Check YCbCr histogram
> You can [check the YCbCr histogram with ffmpeg.](/encoding_pratical_examples.md#generates-yuv-histogram) This scene has a higher blue contribution, which is showed by the [histogram](https://en.wikipedia.org/wiki/Histogram).
>
> ![ycbcr color histogram](/i/yuv_histogram.png "ycbcr color histogram")

### Color, luma, luminance, gama video review

Watch this incredible video explaining what is luma and learn about luminance, gamma, and color.
[![Analog Luma - A history and explanation of video](http://img.youtube.com/vi/Ymt47wXUDEU/0.jpg)](http://www.youtube.com/watch?v=Ymt47wXUDEU)

## Frame types

Now we can move on and try to eliminate the **redundancy in time** but before that let's establish some basic terminology. Suppose we have a movie with 30fps, here are its first 4 frames.

![ball 1](/i/smw_background_ball_1.png "ball 1") ![ball 2](/i/smw_background_ball_2.png "ball 2") ![ball 3](/i/smw_background_ball_3.png "ball 3")
![ball 4](/i/smw_background_ball_4.png "ball 4")

We can see **lots of repetitions** within frames like **the blue background**, it doesn't change from frame 0 to frame 3. To tackle this problem, we can **abstractly categorize** them as three types of frames.

### I Frame (intra, keyframe)

An I-frame (reference, keyframe, intra) is a **self-contained frame**. It doesn't rely on anything to be rendered, an I-frame looks similar to a static photo. The first frame is usually an I-frame but we'll see I-frames inserted regularly among other types of frames.

![ball 1](/i/smw_background_ball_1.png "ball 1")

### P Frame (predicted)

A P-frame takes advantage of the fact that almost always the current picture can be **rendered using the previous frame.** For instance, in the second frame, the only change was the ball that moved forward. We can **rebuild frame 1, only using the difference and referencing to the previous frame**.

![ball 1](/i/smw_background_ball_1.png "ball 1") <-  ![ball 2](/i/smw_background_ball_2_diff.png "ball 2")

> #### Hands-on: A video with a single I-frame
> Since a P-frame uses less data why can't we encode an entire [video with a single I-frame and all the rest being P-frames?](/encoding_pratical_examples.md#1-i-frame-and-the-rest-p-frames)
>
> After you encoded this video, start to watch it and do a **seek for an advanced** part of the video, you'll notice **it takes some time** to really move to that part. That's because a **P-frame needs a reference frame** (I-frame for instance) to be rendered.
>
> Another quick test you can do is to encode a video using a single I-Frame and then [encode it inserting an I-frame each 2s](/encoding_pratical_examples.md#1-i-frames-per-second-vs-05-i-frames-per-second) and **check the size of each rendition**.

### B Frame (bi-predictive)

What about referencing the past and future frames to provide even a better compression?! That's basically what a B-frame is.

![ball 1](/i/smw_background_ball_1.png "ball 1") <-  ![ball 2](/i/smw_background_ball_2_diff.png "ball 2") -> ![ball 3](/i/smw_background_ball_3.png "ball 3")

> #### Hands-on: Compare videos with B-frame
> You can generate two renditions, first with B-frames and other with [no B-frames at all](/encoding_pratical_examples.md#no-b-frames-at-all) and check the size of the file as well as the quality.

### Summary

These frames types are used to **provide better compression**. We'll look how this happens in the next section, but for now we can think of **I-frame as expensive while P-frame is cheaper but the cheapest is the B-frame.**

![frame types example](/i/frame_types.png "frame types example")

## Temporal redundancy (inter prediction)

Let's explore the options we have to reduce the **repetitions in time**, this type of redundancy can be solved with techniques of **inter prediction**.


We will try to **spend fewer bits** to encode the sequence of frames 0 and 1.

![original frames](/i/original_frames.png "original frames")

One thing we can do it's a subtraction, we simply **subtract frame 1 from frame 0** and we get just what we need to **encode the residual**.

![delta frames](/i/difference_frames.png "delta frames")

But what if I tell you that there is a **better method** which uses even fewer bits?! First, let's treat the `frame 0` as a collection of well-defined partitions and then we'll try to match the blocks from `frame 0` on `frame 1`. We can think of it as **motion estimation**.

> ### Wikipedia - block motion compensation
> "**Block motion compensation** divides up the current frame into non-overlapping blocks, and the motion compensation vector **tells where those blocks come from** (a common misconception is that the previous frame is divided up into non-overlapping blocks, and the motion compensation vectors tell where those blocks move to). The source blocks typically overlap in the source frame. Some video compression algorithms assemble the current frame out of pieces of several different previously-transmitted frames."

![delta frames](/i/original_frames_motion_estimation.png "delta frames")

We could estimate that the ball moved from `x=0, y=25` to `x=6, y=26`, the **x** and **y** values are the **motion vectors**. One **further step** we can do to save bits is to **encode only the motion vector difference** between the last block position and the predicted, so the final motion vector would be `x=6 (6-0), y=1 (26-25)`

> In a real-world situation, this **ball would be sliced into n partitions** but the process is the same.

The objects on the frame **move in a 3D way**, the ball can become smaller when it moves to the background. It's normal that **we won't find the perfect match** to the block we tried to find a match. Here's a superposed view of our estimation vs the real picture.

![motion estimation](/i/motion_estimation.png "motion estimation")

But we can see that when we apply **motion estimation** the **data to encode is smaller** than using simply delta frame techniques.

![motion estimation vs delta ](/i/comparison_delta_vs_motion_estimation.png "motion estimation delta")

> ### How real motion compensation would look
> This technique is applied to all blocks, very often a ball would be partitioned in more than one block.
>  ![real world motion compensation](/i/real_world_motion_compensation.png "real world motion compensation")
> Source: https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf

You can [play around with these concepts using jupyter](/frame_difference_vs_motion_estimation_plus_residual.ipynb).

> #### Hands-on: See the motion vectors
> We can [generate a video with the inter prediction (motion vectors)  with ffmpeg.](/encoding_pratical_examples.md#generate-debug-video)
>
> ![inter prediction (motion vectors) with ffmpeg](/i/motion_vectors_ffmpeg.png "inter prediction (motion vectors) with ffmpeg")
>
> Or we can use the [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (which is paid but there is a free trial version which limits you to only the first 10 frames).
>
> ![inter prediction intel video pro analyzer](/i/inter_prediction_intel_video_pro_analyzer.png "inter prediction intel video pro analyzer")

## Spatial redundancy (intra prediction)

If we analyze **each frame** in a video we'll see that there are also **many areas that are correlated**.

![](/i/repetitions_in_space.png)

Let's walk through an example. This scene is mostly composed of blue and white colors.

![](/i/smw_bg.png)

This is an `I-frame` and we **can't use previous frames** to predict from but we still can compress it. We will encode the red block selection. If we **look at its neighbors**, we can **estimate** that there is a **trend of colors around it**.

![](/i/smw_bg_block.png)

We will **predict** that the frame will continue to **spread the colors vertically**, it means that the colors of the **unknown pixels will hold the values of its neighbors**.

![](/i/smw_bg_prediction.png)

Our **prediction can be wrong**, for that reason we need to apply this technique (**intra prediction**) and then **subtract the real values** which gives us the residual block, resulting in a much more compressible matrix compared to the original.

![](/i/smw_residual.png)

> #### Hands-on: Check intra predictions
> You can [generate a video with macro blocks and their predictions with ffmpeg.](/encoding_pratical_examples.md#generate-debug-video) Please check the ffmpeg documentation to understand the [meaning of each block color](https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors#AnalyzingMacroblockTypes).
>
> ![intra prediction (macro blocks) with ffmpeg](/i/macro_blocks_ffmpeg.png "inter prediction (motion vectors) with ffmpeg")
>
> Or we can use the [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (which is paid but there is a free trial version which limits you to only the first 10 frames).
>
> ![intra prediction intel video pro analyzer](/i/intra_prediction_intel_video_pro_analyzer.png "intra prediction intel video pro analyzer")

# How does a video codec work?

## What? Why? How?

**What?** It's a piece of software / hardware that compresses or decompresses digital video. **Why?** Market and society demands higher quality videos with limited bandwidth or storage. Remember when we [calculated the needed bandwidth](#basic-terminology) for 30 frames per second, 24 bits per pixel, resolution of a 480x240 video? It was **82.944 Mbps** with no compression applied. It's the only way to deliver HD/FullHD/4K in TVs and the Internet. **How?** We'll take a brief look at the major techniques here.

> **CODEC vs Container**
>
> One common mistake that beginners often do is to confuse digital video CODEC and [digital video container](https://en.wikipedia.org/wiki/Digital_container_format). We can think of **containers** as a wrapper format which contains metadata of the video (and possible audio too), and the **compressed video** can be seen as its payload.
>
> Usually, the extension of a video file defines its video container. For instance, the file `video.mp4` is probably a **[MPEG-4 Part 14](https://en.wikipedia.org/wiki/MPEG-4_Part_14)** container and a file named `video.mkv` it's probably a **[matroska](https://en.wikipedia.org/wiki/Matroska)**. To be completely sure about the codec and container format we can use [ffmpeg or mediainfo](/encoding_pratical_examples.md#inspect-stream).

## History

Before we jump into the inner workings of a generic codec, let's look back to understand a little better about some old video codecs.

The video codec [H.261](https://en.wikipedia.org/wiki/H.261)  was born in 1990 (technically 1988), and it was designed to work with **data rates of 64 kbit/s**. It already uses ideas such as chroma subsampling, macro block, etc. In the year of 1995, the **H.263** video codec standard was published and continued to be extended until 2001.

In 2003 the first version of **H.264/AVC** was completed. In the same year, a company called **TrueMotion** released their video codec as a **royalty-free** lossy video compression called **VP3**. In 2008, **Google bought** this company, releasing **VP8** in the same year. In December of 2012, Google released the **VP9** and it's  **supported by roughly ¬æ of the browser market** (mobile included).

 **[AV1](https://en.wikipedia.org/wiki/AOMedia_Video_1)** is a new **royalty-free** and open source video codec that's being designed by the [Alliance for Open Media (AOMedia)](http://aomedia.org/), which is composed of the **companies: Google, Mozilla, Microsoft, Amazon, Netflix, AMD, ARM, NVidia, Intel and Cisco** among others. The **first version** 0.1.0 of the reference codec was **published on April 7, 2016**.

![codec history timeline](/i/codec_history_timeline.png "codec history timeline")

> #### The birth of AV1
>
> Early 2015, Google was working on [VP10](https://en.wikipedia.org/wiki/VP9#Successor:_from_VP10_to_AV1), Xiph (Mozilla) was working on [Daala](https://xiph.org/daala/) and Cisco open-sourced its royalty-free video codec called [Thor](https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03).
>
> Then MPEG LA first announced annual caps for HEVC (H.265) and fees 8 times higher than H.264 but soon they changed the rules again:
> * **no annual cap**,
> * **content fee** (0.5% of revenue) and
> * **per-unit fees about 10 times higher than h264**.
>
> The [alliance for open media](http://aomedia.org/about-us/) was created by companies from hardware manufacturer (Intel, AMD, ARM , Nvidia, Cisco), content delivery (Google, Netflix, Amazon), browser maintainers (Google, Mozilla), and others.
>
> The companies had a common goal, a royalty-free video codec and then AV1 was born with a much [simpler patent license](http://aomedia.org/license/patent/). **Timothy B. Terriberry** did an awesome presentation, which is the source of this section, about the [AV1 conception, license model and its current state](https://www.youtube.com/watch?v=lzPaldsmJbk).
>
> You'll be surprised to know that you can **analyze the AV1 codec through your browser**, go to http://aomanalyzer.org/
>
> ![av1 browser analyzer](/i/av1_browser_analyzer.png "av1 browser analyzer")
>
> PS: If you want to learn more about the history of the codecs you must learn the basics behind [video compression patents](https://www.vcodex.com/video-compression-patents/).

## A generic codec

We're going to introduce the **main mechanics behind a generic video codec** but most of these concepts are useful and used in modern codecs such as VP9, AV1 and HEVC. Be sure to understand that we're going to simplify things a LOT. Sometimes we'll use a real example (mostly H.264) to demonstrate a technique.

## 1st step - picture partitioning

The first step is to **divide the frame** into several **partitions, sub-partitions** and beyond.

![picture partitioning](/i/picture_partitioning.png "picture partitioning")

**But why?** There are many reasons, for instance, when we split the picture we can work the predictions more precisely, using small partitions for the small moving parts while using bigger partitions to a static background.

Usually, the CODECs **organize these partitions** into slices (or tiles), macro (or coding tree units) and many sub-partitions. The max size of these partitions varies, HEVC sets 64x64 while AVC uses 16x16 but the sub-partitions can reach sizes of 4x4.

Remember that we learned how **frames are typed**?! Well, you can **apply those ideas to blocks** too, therefore we can have I-Slice, B-Slice, I-Macroblock and etc.

> ### Hands-on: Check partitions
> We can also use the [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) (which is paid but there is a free trial version which limits you to only the first 10 frames). Here are [VP9 partitions](/encoding_pratical_examples.md#transcoding) analyzed.
>
> ![VP9 partitions view intel video pro analyzer ](/i/paritions_view_intel_video_pro_analyzer.png "VP9 partitions view intel video pro analyzer")

## 2nd step - predictions

Once we have the partitions, we can make predictions over them. For the [inter prediction](#temporal-redundancy-inter-prediction) we need **to send the motion vectors and the residual** and the [intra prediction](#spatial-redundancy-intra-prediction) we'll **send the prediction direction and the residual** as well.

## 3rd step - transform

After we get the residual block (`predicted partition - real partition`), we can **transform** it in a way that lets us know which **pixels we can discard** while keeping the **overall quality**. There are some transformations for this exact behavior.

Although there are [other transformations](https://en.wikipedia.org/wiki/List_of_Fourier-related_transforms#Discrete_transforms), we'll look more closely at the discrete cosine transform (DCT). The [**DCT**](https://en.wikipedia.org/wiki/Discrete_cosine_transform) main features are:

* **converts** blocks of **pixels** into  same-sized blocks of **frequency coefficients**.
* **compacts** energy, making it easy to eliminate spatial redundancy.
* is **reversible**, a.k.a. you can reverse to pixels.

> On 2 Feb 2017, Cintra, R. J. and Bayer, F. M have published their paper [DCT-like Transform for Image Compression
Requires 14 Additions Only](https://arxiv.org/abs/1702.00817).

Don't worry if you didn't understand the benefits from every bullet point, we'll try to make some experiments in order to see the real value from it.

Let's take the following **block of pixels** (8x8):

![pixel values matrix](/i/pixel_matrice.png "pixel values matrix")

Which renders to the following block image (8x8):

![pixel values matrix](/i/gray_image.png "pixel values matrix")

When we **apply the DCT** over this block of pixels and we get the **block of coefficients** (8x8):

![coefficients values](/i/dct_coefficient_values.png "coefficients values")

And if we render this block of coefficients, we'll get this image:

![dct coefficients image](/i/dct_coefficient_image.png "dct coefficients image")

As you can see it looks nothing like the original image, we might notice that the **first coefficient** is very different from all the others. This first coefficient is known as the DC coefficient which represents of **all the samples** in the input array, something **similar to an average**.

This block of coefficients has an interesting property which is that it separates the high-frequency components from the low frequency.

![dct frequency coefficients property](/i/dctfrequ.jpg "dct frequency coefficients property")

In an image, **most of the energy** will be concentrated in the [**lower frequencies**](https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm), so if we transform an image into its frequency components and **throw away the higher frequency coefficients**, we can **reduce the amount of data** needed to describe the image without sacrificing too much image quality.

> frequency means how fast a signal is changing

Let's try to apply the knowledge we acquired in the test by converting the original image to its frequency (block of coefficients) using DCT and then throwing away part of the least important coefficients.

First, we convert it to its **frequency domain**.

![coefficients values](/i/dct_coefficient_values.png "coefficients values")

Next, we discard part (67%) of the coefficients, mostly the bottom right part of it.

![zeroed coefficients](/i/dct_coefficient_zeroed.png "zeroed coefficients")

Finally, we reconstruct the image from this discarded block of coefficients (remember, it needs to be reversible) and compare it to the original.

![original vs quantized](/i/original_vs_quantized.png "original vs quantized")

As we can see it resembles the original image but it introduced lots of differences from the original, we **throw away 67.1875%** and we still were able to get at least something similar to the original. We could more intelligently discard the coefficients to have a better image quality but that's the next topic.

> **Each coefficient is formed using all the pixels**
>
> It's important to note that each coefficient doesn't directly map to a single pixel but it's a weighted sum of all pixels. This amazing graph shows how the first and second coefficient is calculated, using weights which are unique for each index.
>
> ![dct calculation](/i/applicat.jpg "dct calculation")
>
> Source: https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm
>
> You can also try to [visualize the DCT by looking at a simple image](/dct_better_explained.ipynb) formation over the DCT basis. For instance, here's the [A character being formed](https://en.wikipedia.org/wiki/Discrete_cosine_transform#Example_of_IDCT) using each coefficient weight.
>
> ![](https://upload.wikimedia.org/wikipedia/commons/5/5e/Idct-animation.gif )




<br/>

> ### Hands-on: throwing away different coefficients
> You can play around with the [DCT transform](/uniform_quantization_experience.ipynb).

## 4th step - quantization

When we throw away some of the coefficients, in the last step (transform), we kinda did some form of quantization. This step is where we chose to lose information (the **lossy part**) or in simple terms, we'll **quantize coefficients to achieve compression**.

How can we quantize a block of coefficients? One simple method would be a uniform quantization, where we take a block, **divide it by a single value** (10) and round this value.

![quantize](/i/quantize.png "quantize")

How can we **reverse** (re-quantize) this block of coefficients? We can do that by **multiplying the same value** (10) we divide it first.

![re-quantize](/i/re-quantize.png "re-quantize")

This **approach isn't the best** because it doesn't take into account the importance of each coefficient, we could use a **matrix of quantizers** instead of a single value, this matrix can exploit the property of the DCT, quantizing most the bottom right and less the upper left, the [JPEG uses a similar approach](https://www.hdm-stuttgart.de/~maucher/Python/MMCodecs/html/jpegUpToQuant.html), you can check [source code to see this matrix](https://github.com/google/guetzli/blob/master/guetzli/jpeg_data.h#L40).

> ### Hands-on: quantization
> You can play around with the [quantization](/dct_experiences.ipynb).

## 5th step - entropy coding

After we quantized the data (image blocks/slices/frames) we still can compress it in a lossless way. There are many ways (algorithms) to compress data. We're going to briefly experience some of them, for a deeper understanding you can read the amazing book [Understanding Compression: Data Compression for Modern Developers](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/).

### VLC coding:

Let's suppose we have a stream of the symbols: **a**, **e**, **r** and **t** and their probability (from 0 to 1) is represented by this table.

|             | a   | e   | r    | t   |
|-------------|-----|-----|------|-----|
| probability | 0.3 | 0.3 | 0.2 |  0.2 |

We can assign unique binary codes (preferable small) to the most probable and bigger codes to the least probable ones.

|             | a   | e   | r    | t   |
|-------------|-----|-----|------|-----|
| probability | 0.3 | 0.3 | 0.2 | 0.2 |
| binary code | 0 | 10 | 110 | 1110 |

Let's compress the stream **eat**, assuming we would spend 8 bits for each symbol, we would spend **24 bits** without any compression. But in case we replace each symbol for its code we can save space.

The first step is to encode the symbol **e** which is `10` and the second symbol is **a** which is added (not in a mathematical way) `[10][0]` and finally the third symbol **t** which makes our final compressed bitstream to be `[10][0][1110]` or `1001110` which only requires **7 bits** (3.4 times less space than the original).

Notice that each code must be a unique prefixed code [Huffman can help you to find these numbers](https://en.wikipedia.org/wiki/Huffman_coding). Though it has some issues there are [video codecs that still offers](https://en.wikipedia.org/wiki/Context-adaptive_variable-length_coding) this method and it's the  algorithm for many applications which requires compression.

Both encoder and decoder **must know** the symbol table with its code, therefore, you need to send the table too.

### Arithmetic coding:

Let's suppose we have a stream of the symbols: **a**, **e**, **r**, **s** and **t** and their probability is represented by this table.

|             | a   | e   | r    | s    | t   |
|-------------|-----|-----|------|------|-----|
| probability | 0.3 | 0.3 | 0.15 | 0.05 | 0.2 |

With this table in mind, we can build ranges containing all the possible symbols sorted by the most frequents.

![initial arithmetic range](/i/range.png "initial arithmetic range")

Now let's encode the stream **eat**, we pick the first symbol **e** which is located within the subrange **0.3 to 0.6** (but not included) and we take this subrange and split it again using the same proportions used before but within this new range.

![second sub range](/i/second_subrange.png "second sub range")

Let's continue to encode our stream **eat**, now we take the second symbol **a** which is within the new subrange **0.3 to 0.39** and then we take our last symbol **t** and we do the same process again and we get the last subrange **0.354 to 0.372**.

![final arithmetic range](/i/arithimetic_range.png "final arithmetic range")

We just need to pick a number within the last subrange **0.354 to 0.372**, let's choose **0.36** but we could choose any number within this subrange. With **only** this number we'll be able to recover our original stream **eat**. If you think about it, it's like if we were drawing a line within ranges of ranges to encode our stream.

![final range traverse](/i/range_show.png "final range traverse")

The **reverse process** (A.K.A. decoding) is equally easy, with our number **0.36** and our original range we can run the same process but now using this number to reveal the stream encoded behind this number.

With the first range, we notice that our number fits at the slice, therefore, it's our first symbol, now we split this subrange again, doing the same process as before, and we'll notice that **0.36** fits the symbol **a** and after we repeat the process we came to the last symbol **t** (forming our original encoded stream *eat*).

Both encoder and decoder **must know** the symbol probability table, therefore you need to send the table.

Pretty neat, isn't it? People are damn smart to come up with a such solution, some [video codecs use](https://en.wikipedia.org/wiki/Context-adaptive_binary_arithmetic_coding) this technique (or at least offer it as an option).

The idea is to lossless compress the quantized bitstream, for sure this article is missing tons of details, reasons, trade-offs and etc. But [you should learn more](https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/) as a developer. Newer codecs are trying to use different [entropy coding algorithms like ANS.](https://en.wikipedia.org/wiki/Asymmetric_Numeral_Systems)

> ### Hands-on: CABAC vs CAVLC
> You can [generate two streams, one with CABAC and other with CAVLC](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#cabac-vs-cavlc) and **compare the time** it took to generate each of them as well as **the final size**.

## 6th step - bitstream format

After we did all these steps we need to **pack the compressed frames and context to these steps**. We need to explicitly inform to the decoder about **the decisions taken by the encoder**, such as bit depth, color space, resolution, predictions info (motion vectors, intra prediction direction), profile, level, frame rate, frame type, frame number and much more.

We're going to study, superficially, the H.264 bitstream. Our first step is to [generate a minimal  H.264 <sup>*</sup> bitstream](/encoding_pratical_examples.md#generate-a-single-frame-h264-bitstream), we can do that using our own repository and [ffmpeg](http://ffmpeg.org/).

```
./s/ffmpeg -i /files/i/minimal.png -pix_fmt yuv420p /files/v/minimal_yuv420.h264
```

> <sup>*</sup> ffmpeg adds, by default, all the encoding parameter as a **SEI NAL**, soon we'll define what is a NAL.

This command will generate a raw h264 bitstream with a **single frame**, 64x64, with color space yuv420 and using the following image as the frame.

> ![used frame to generate minimal h264 bitstream](/i/minimal.png "used frame to generate minimal h264 bitstream")

### H.264 bitstream

The AVC (H.264) standard defines that the information will be sent in **macro frames** (in the network sense), called **[NAL](https://en.wikipedia.org/wiki/Network_Abstraction_Layer)** (Network Abstraction Layer). The main goal of the NAL is the provision of a "network-friendly" video representation, this standard must work on TVs (stream based), the Internet (packet based) among others.

![NAL units H.264](/i/nal_units.png "NAL units H.264")

There is a **[synchronization marker](https://en.wikipedia.org/wiki/Frame_synchronization)** to define the boundaries of the NAL's units. Each synchronization marker holds a value of `0x00 0x00 0x01` except to the very first one which is `0x00 0x00 0x00 0x01`. If we run the **hexdump** on the generated h264 bitstream, we can identify at least three NALs in the beginning of the file.

![synchronization marker on NAL units](/i/minimal_yuv420_hex.png "synchronization marker on NAL units")

As we said before, the decoder needs to know not only the picture data but also the details of the video, frame, colors, used parameters, and others. The **first byte** of each NAL defines its category and **type**.

| NAL type id  | Description  |
|---  |---|
| 0  |  Undefined |
| 1  |  Coded slice of a non-IDR picture |
| 2  |  Coded slice data partition A |
| 3  |  Coded slice data partition B |
| 4  |  Coded slice data partition C |
| 5  |  **IDR** Coded slice of an IDR picture |
| 6  |  **SEI** Supplemental enhancement information |
| 7  |  **SPS** Sequence parameter set |
| 8  |  **PPS** Picture parameter set |
| 9  |  Access unit delimiter |
| 10 |  End of sequence |
| 11 |  End of stream |
| ... |  ... |

Usually, the first NAL of a bitstream is a **SPS**, this type of NAL is responsible for informing the general encoding variables like **profile**, **level**, **resolution** and others.

If we skip the first synchronization marker we can decode the **first byte** to know what **type of NAL** is the first one.

For instance the first byte after the synchronization marker is `01100111`, where the first bit (`0`) is to the field **forbidden_zero_bit**, the next 2 bits (`11`) tell us the field **nal_ref_idc** which indicates whether this NAL is a reference field or not and the rest 5 bits (`00111`) inform us the field **nal_unit_type**, in this case, it's a **SPS** (7) NAL unit.

The second byte (`binary=01100100, hex=0x64, dec=100`) of an SPS NAL is the field **profile_idc** which shows the profile that the encoder has used, in this case, we used  the **[constrained high-profile](https://en.wikipedia.org/wiki/H.264/MPEG-4_AVC#Profiles)**, it's a high profile without the support of B (bi-predictive) slices.

![SPS binary view](/i/minimal_yuv420_bin.png "SPS binary view")

When we read the H.264 bitstream spec for an SPS NAL we'll find many values for the **parameter name**, **category** and a **description**, for instance, let's look at `pic_width_in_mbs_minus_1` and `pic_height_in_map_units_minus_1` fields.

| Parameter name  | Category  |  Description  |
|---  |---|---|
| pic_width_in_mbs_minus_1 |  0 | ue(v) |
| pic_height_in_map_units_minus_1 |  0 | ue(v) |

> **ue(v)**: unsigned integer [Exp-Golomb-coded](https://pythonhosted.org/bitstring/exp-golomb.html)

If we do some math with the value of these fields we will end up with the **resolution**. We can represent a `1920 x 1080` using a `pic_width_in_mbs_minus_1` with the value of `119 ( (119 + 1) * macroblock_size = 120 * 16 = 1920) `, again saving space, instead of encode `1920` we did it with `119`.

If we continue to examine our created video with a binary view (ex: `xxd -b -c 11 v/minimal_yuv420.h264`), we can skip to the last NAL which is the frame itself.

![h264 idr slice header](/i/slice_nal_idr_bin.png "h264 idr slice header")

We can see its first 6 bytes values: `01100101 10001000 10000100 00000000 00100001 11111111`. As we already know the first byte tell us about what type of NAL it is, in this case, (`00101`) it's an **IDR Slice (5)** and we can further inspect it:

![h264 slice header spec](/i/slice_header.png "h264 slice header spec")

Using the spec info we can decode what type of slice (**slice_type**), the frame number (**frame_num**) among others important fields.

In order to get the values of some fields (`ue(v), me(v), se(v) or te(v)`) we need to decode it using a special decoder called [Exponential-Golomb](https://pythonhosted.org/bitstring/exp-golomb.html), this method is **very efficient to encode variable values**, mostly when there are many default values.

> The values of **slice_type** and **frame_num** of this video are 7 (I slice) and 0 (the first frame).

We can see the **bitstream as a protocol** and if you want or need to learn more about this bitstream please refer to the [ITU H.264 spec.]( http://www.itu.int/rec/T-REC-H.264-201610-I) Here's a macro diagram which shows where the picture data (compressed YUV) resides.

![h264 bitstream macro diagram](/i/h264_bitstream_macro_diagram.png "h264 bitstream macro diagram")

We can explore others bitstreams like the [VP9 bitstream](https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf), [H.265 (HEVC)](http://handle.itu.int/11.1002/1000/11885-en?locatt=format:pdf) or even our **new best friend** [**AV1** bitstream](https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8
), [do they all look similar? No](http://www.gpac-licensing.com/2016/07/12/vp9-av1-bitstream-format/), but once you learned one you can easily get the others.

> ### Hands-on: Inspect the H.264 bitstream
> We can [generate a single frame video](https://github.com/leandromoreira/introduction_video_technology/blob/master/encoding_pratical_examples.md#generate-a-single-frame-video) and use  [mediainfo](https://en.wikipedia.org/wiki/MediaInfo) to inspect its H.264 bitstream. In fact, you can even see the [source code that parses h264 (AVC)](https://github.com/MediaArea/MediaInfoLib/blob/master/Source/MediaInfo/Video/File_Avc.cpp) bitstream.
>
> ![mediainfo details h264 bitstream](/i/mediainfo_details_1.png "mediainfo details h264 bitstream")
>
> We can also use the [Intel Video Pro Analyzer](https://software.intel.com/en-us/intel-video-pro-analyzer) which is paid but there is a free trial version which limits you to only the first 10 frames but that's okay for learning purposes.
>
> ![intel video pro analyzer details h264 bitstream](/i/intel-video-pro-analyzer.png "intel video pro analyzer details h264 bitstream")

## Review

We'll notice that many of the **modern codecs uses this same model we learned**. In fact, let's look at the Thor video codec block diagram, it contains all the steps we studied. The idea is that you now should be able to at least understand better the innovations and papers for the area.

![thor_codec_block_diagram](/i/thor_codec_block_diagram.png "thor_codec_block_diagram")

Previously we had calculated that we needed [139GB of storage to keep a video file with one hour at 720p resolution and 30fps](#chroma-subsampling) if we use the techniques we learned here, like **inter and intra prediction, transform, quantization, entropy coding and other** we can achieve, assuming we are spending **0.031 bit per pixel**, the same perceivable quality video **requiring only 367.82MB vs 139GB** of store.

> We choose to use **0.031 bit per pixel** based on the example video provided here.

## How does H.265 achieve a better compression ratio than H.264?

Now that we know more about how codecs work, then it is easy to understand how new codecs are able to deliver higher resolutions with fewer bits.

We will compare AVC and HEVC, let's keep in mind that it is almost always a trade-off between more CPU cycles (complexity) and compression rate.

HEVC has bigger and more **partitions** (and **sub-partitions**) options than AVC, more **intra predictions directions**, **improved entropy coding** and more, all these improvements made H.265 capable to compress 50% more than H.264.

![h264 vs h265](/i/avc_vs_hevc.png "H.264 vs H.265")

# Online streaming
## General architecture

![general architecture](/i/general_architecture.png "general architecture")

[TODO]

## Progressive download and adaptive streaming

![progressive download](/i/progressive_download.png "progressive download")

![adaptive streaming](/i/adaptive_streaming.png "adaptive streaming")

[TODO]

## Content protection

We can use **a simple token system** to protect the content. The user without a token tries to request a video and the CDN forbids her or him while a user with a valid token can play the content, it works pretty similarly to most of the web authentication systems.

![token protection](/i/token_protection.png "token_protection")

The sole use of this token system still allows a user to download a video and distribute it. Then the **DRM (digital rights management)** systems can be used to try to avoid this.

![drm](/i/drm.png "drm")

In real life production systems, people often use both techniques to provide authorization and authentication.

### DRM
#### Main systems

* FPS - [**FairPlay Streaming**](https://developer.apple.com/streaming/fps/)
* PR - [**PlayReady**](https://www.microsoft.com/playready/)
* WV - [**Widevine**](http://www.widevine.com/)


#### What?

DRM means Digital rights management, it's a way **to provide copyright protection for digital media**, for instance, digital video and audio. Although it's used in many places [it's not universally accepted](https://en.wikipedia.org/wiki/Digital_rights_management#DRM-free_works).

#### Why?

Content creator (mostly studios) want to protect its intelectual property against copy to prevent unauthorized redistribution of digital media.

#### How?

We're going to describe an abstract and generic form of DRM in a very simplified way.

Given a **content C1** (i.e. an hls or dash video streaming), with a **player P1** (i.e. shaka-clappr, exo-player or ios) in a **device D1** (i.e. a smartphone, TV, tablet or desktop/notebook) using a **DRM system DRM1** (widevine, playready or FairPlay).

The content C1 is encrypted with a **symmetric-key K1** from the system DRM1, generating the **encrypted content C'1**.

![drm general flow](/i/drm_general_flow.jpeg "drm general flow")

The player P1, of a device D1, has two keys (asymmetric), a **private key PRK1** (this key is protected<sup>1</sup> and only known by **D1**) and a **public key PUK1**.

> **<sup>1</sup>protected**: this protection can be **via hardware**, for instance, this key can be stored inside a special (read-only) chip that works like [a black-box](https://en.wikipedia.org/wiki/Black_box) to provide decryption, or **by software** (less safe), the DRM system provides means to know which type of protection a given device has.


When the **player P1 wants to play** the **content C'1**, it needs to deal with the **DRM system DRM1**, giving its public key **PUK1**. The DRM system DRM1 returns the **key K1 encrypted** with the client''s public key **PUK1**. In theory, this response is something that **only D1 is capable of decrypting**.

`K1P1D1 = enc(K1, PUK1)`

**P1** uses its DRM local system (it could be a [SOC](https://en.wikipedia.org/wiki/System_on_a_chip), a specialized hardware or software), this system is **able to decrypt** the content using its private key PRK1, it can decrypt **the symmetric-key K1 from the K1P1D1** and **play C'1**. At best case, the keys are not exposed through RAM.

 ```
 K1 = dec(K1P1D1, PRK1)

 P1.play(dec(C'1, K1))
 ```

![drm decoder flow](/i/drm_decoder_flow.jpeg "drm decoder flow")

# How to use jupyter

Make sure you have **docker installed** and just run `./s/start_jupyter.sh` and follow the instructions on the terminal.

# Conferences

* [DEMUXED](https://demuxed.com/) - you can [check the last 2 events presentations.](https://www.youtube.com/channel/UCIc_DkRxo9UgUSTvWVNCmpA).

# References

The richest content is here, it's where all the info we saw in this text was extracted, based or inspired by. You can deepen your knowledge with these amazing links, books, videos and etc.

Online Courses and Tutorials:

* https://www.coursera.org/learn/digital/
* https://people.xiph.org/~tterribe/pubs/lca2012/auckland/intro_to_video1.pdf
* https://xiph.org/video/vid1.shtml
* https://xiph.org/video/vid2.shtml
* http://slhck.info/ffmpeg-encoding-course
* http://www.cambridgeincolour.com/tutorials/camera-sensors.htm
* http://www.slideshare.net/vcodex/a-short-history-of-video-coding
* http://www.slideshare.net/vcodex/introduction-to-video-compression-13394338
* https://developer.android.com/guide/topics/media/media-formats.html
* http://www.slideshare.net/MadhawaKasun/audio-compression-23398426
* http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/02-Motion_Compensation_girod.pdf

Books:

* https://www.amazon.com/Understanding-Compression-Data-Modern-Developers/dp/1491961538/ref=sr_1_1?s=books&ie=UTF8&qid=1486395327&sr=1-1
* https://www.amazon.com/H-264-Advanced-Video-Compression-Standard/dp/0470516925
* https://www.amazon.com/Practical-Guide-Video-Audio-Compression/dp/0240806301/ref=sr_1_3?s=books&ie=UTF8&qid=1486396914&sr=1-3&keywords=A+PRACTICAL+GUIDE+TO+VIDEO+AUDIO
* https://www.amazon.com/Video-Encoding-Numbers-Eliminate-Guesswork/dp/0998453005/ref=sr_1_1?s=books&ie=UTF8&qid=1486396940&sr=1-1&keywords=jan+ozer

Onboarding material:

* https://github.com/Eyevinn/streaming-onboarding
* https://howvideo.works/
* https://www.aws.training/Details/eLearning?id=17775
* https://www.aws.training/Details/eLearning?id=17887
* https://www.aws.training/Details/Video?id=24750

Bitstream Specifications:

* http://www.itu.int/rec/T-REC-H.264-201610-I
* http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=12904&lang=en
* https://storage.googleapis.com/downloads.webmproject.org/docs/vp9/vp9-bitstream-specification-v0.6-20160331-draft.pdf
* http://iphome.hhi.de/wiegand/assets/pdfs/2012_12_IEEE-HEVC-Overview.pdf
* http://phenix.int-evry.fr/jct/doc_end_user/current_document.php?id=7243
* http://gentlelogic.blogspot.com.br/2011/11/exploring-h264-part-2-h264-bitstream.html
* https://forum.doom9.org/showthread.php?t=167081
* https://forum.doom9.org/showthread.php?t=168947

Software:

* https://ffmpeg.org/
* https://ffmpeg.org/ffmpeg-all.html
* https://ffmpeg.org/ffprobe.html
* https://trac.ffmpeg.org/wiki/
* https://software.intel.com/en-us/intel-video-pro-analyzer
* https://medium.com/@mbebenita/av1-bitstream-analyzer-d25f1c27072b#.d5a89oxz8

Non-ITU Codecs:

* https://aomedia.googlesource.com/
* https://github.com/webmproject/libvpx/tree/master/vp9
* https://people.xiph.org/~xiphmont/demo/daala/demo1.shtml
* https://people.xiph.org/~jm/daala/revisiting/
* https://www.youtube.com/watch?v=lzPaldsmJbk
* https://fosdem.org/2017/schedule/event/om_av1/
* https://jmvalin.ca/papers/AV1_tools.pdf

Encoding Concepts:

* http://x265.org/hevc-h265/
* http://slhck.info/video/2017/03/01/rate-control.html
* http://slhck.info/video/2017/02/24/vbr-settings.html
* http://slhck.info/video/2017/02/24/crf-guide.html
* https://arxiv.org/pdf/1702.00817v1.pdf
* https://trac.ffmpeg.org/wiki/Debug/MacroblocksAndMotionVectors
* http://web.ece.ucdavis.edu/cerl/ReliableJPEG/Cung/jpeg.html
* http://www.adobe.com/devnet/adobe-media-server/articles/h264_encoding.html
* https://prezi.com/8m7thtvl4ywr/mp3-and-aac-explained/
* https://blogs.gnome.org/rbultje/2016/12/13/overview-of-the-vp9-video-codec/
* https://videoblerg.wordpress.com/2017/11/10/ffmpeg-and-how-to-use-it-wrong/

Video Sequences for Testing:

* http://bbb3d.renderfarming.net/download.html
* https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx

Miscellaneous:

* https://github.com/Eyevinn/streaming-onboarding
* http://stackoverflow.com/a/24890903
* http://stackoverflow.com/questions/38094302/how-to-understand-header-of-h264
* http://techblog.netflix.com/2016/08/a-large-scale-comparison-of-x264-x265.html
* http://vanseodesign.com/web-design/color-luminance/
* http://www.biologymad.com/nervoussystem/eyenotes.htm
* http://www.compression.ru/video/codec_comparison/h264_2012/mpeg4_avc_h264_video_codecs_comparison.pdf
* http://www.csc.villanova.edu/~rschumey/csc4800/dct.html
* http://www.explainthatstuff.com/digitalcameras.html
* http://www.hkvstar.com
* http://www.hometheatersound.com/
* http://www.lighterra.com/papers/videoencodingh264/
* http://www.red.com/learn/red-101/video-chroma-subsampling
* http://www.slideshare.net/ManoharKuse/hevc-intra-coding
* http://www.slideshare.net/mwalendo/h264vs-hevc
* http://www.slideshare.net/rvarun7777/final-seminar-46117193
* http://www.springer.com/cda/content/document/cda_downloaddocument/9783642147029-c1.pdf
* http://www.streamingmedia.com/Articles/Editorial/Featured-Articles/A-Progress-Report-The-Alliance-for-Open-Media-and-the-AV1-Codec-110383.aspx
* http://www.streamingmediaglobal.com/Articles/ReadArticle.aspx?ArticleID=116505&PageNum=1
* http://yumichan.net/video-processing/video-compression/introduction-to-h264-nal-unit/
* https://cardinalpeak.com/blog/the-h-264-sequence-parameter-set/
* https://cardinalpeak.com/blog/worlds-smallest-h-264-encoder/
* https://codesequoia.wordpress.com/category/video/
* https://developer.apple.com/library/content/technotes/tn2224/_index.html
* https://en.wikibooks.org/wiki/MeGUI/x264_Settings
* https://en.wikipedia.org/wiki/Adaptive_bitrate_streaming
* https://en.wikipedia.org/wiki/AOMedia_Video_1
* https://en.wikipedia.org/wiki/Chroma_subsampling#/media/File:Colorcomp.jpg
* https://en.wikipedia.org/wiki/Cone_cell
* https://en.wikipedia.org/wiki/File:H.264_block_diagram_with_quality_score.jpg
* https://en.wikipedia.org/wiki/Inter_frame
* https://en.wikipedia.org/wiki/Intra-frame_coding
* https://en.wikipedia.org/wiki/Photoreceptor_cell
* https://en.wikipedia.org/wiki/Pixel_aspect_ratio
* https://en.wikipedia.org/wiki/Presentation_timestamp
* https://en.wikipedia.org/wiki/Rod_cell
* https://it.wikipedia.org/wiki/File:Pixel_geometry_01_Pengo.jpg
* https://leandromoreira.com.br/2016/10/09/how-to-measure-video-quality-perception/
* https://sites.google.com/site/linuxencoding/x264-ffmpeg-mapping
* https://softwaredevelopmentperestroika.wordpress.com/2014/02/11/image-processing-with-python-numpy-scipy-image-convolution/
* https://tools.ietf.org/html/draft-fuldseth-netvc-thor-03
* https://www.encoding.com/android/
* https://www.encoding.com/http-live-streaming-hls/
* https://web.archive.org/web/20150129171151/https://www.iem.thm.de/telekom-labor/zinke/mk/mpeg2beg/whatisit.htm
* https://www.lifewire.com/cmos-image-sensor-493271
* https://www.linkedin.com/pulse/brief-history-video-codecs-yoav-nativ
* https://www.linkedin.com/pulse/video-streaming-methodology-reema-majumdar
* https://www.vcodex.com/h264avc-intra-precition/
* https://www.youtube.com/watch?v=9vgtJJ2wwMA
* https://www.youtube.com/watch?v=LFXN9PiOGtY
* https://www.youtube.com/watch?v=Lto-ajuqW3w&list=PLzH6n4zXuckpKAj1_88VS-8Z6yn9zX_P6
* https://www.youtube.com/watch?v=LWxu4rkZBLw
* https://web.stanford.edu/class/ee398a/handouts/lectures/EE398a_MotionEstimation_2012.pdf
